{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"../..\")\n",
    "Pkg.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cholesky Decomposition\n",
    "\n",
    "<img src=\"http://www-groups.dcs.st-and.ac.uk/history/BigPictures/Cholesky_2.jpeg\" width=\"300\" align=\"center\"/>\n",
    "\n",
    "[André-Louis Cholesky, 1875-1918](https://en.wikipedia.org/wiki/André-Louis_Cholesky).\n",
    "\n",
    "* A basic tenet in numerical analysis: \n",
    "\n",
    "> **The structure should be exploited whenever possible in solving a problem.** \n",
    "\n",
    "  Common structures include: symmetry, positive (semi)definiteness, sparsity, Kronecker product, low rank, ...\n",
    "\n",
    "* LU decomposition (Gaussian Elimination) is **not** used in statistics very often because most of time statisticians deal with positive (semi)definite matrices.\n",
    "\n",
    "* Recall that a matrix $M$ is positive (semi)definite if\n",
    "$$\n",
    "    \\mathbf{x}^T\\mathbf{M}\\mathbf{x} > 0 ~(\\ge 0), \\quad \\forall \\mathbf{x}\\neq\\mathbf{0}.\n",
    "$$\n",
    "\n",
    "* Example: normal equation \n",
    "$$\n",
    "    \\mathbf{X}^T \\mathbf{X} \\beta = \\mathbf{X}^T \\mathbf{y}\n",
    "$$\n",
    "for linear regression, the coefficient matrix $\\mathbf{X}^T \\mathbf{X}$ is symmetric and positive semidefinite. How to exploit this structure?\n",
    "\n",
    "* Most of time statisticians deal with positive (semi)definite matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cholesky decomposition\n",
    "\n",
    "* **Theorem**: Let $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ be symmetric and positive definite. Then $\\mathbf{A} = \\mathbf{L} \\mathbf{L}^T$, where $\\mathbf{L}$ is lower triangular with positive diagonal entries and is unique.  \n",
    "**Proof** (by induction):  \n",
    "If $n=1$, then $0 < a = \\sqrt{a}\\sqrt{a}$. For $n>1$, the block equation\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\begin{bmatrix}\n",
    "a_{11} & \\mathbf{a}^T \\\\ \\mathbf{a} & \\mathbf{A}_{22}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "\t\\ell_{11} & \\mathbf{0}_{n-1}^T \\\\ \\mathbf{b} & \\mathbf{L}_{22}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\t\\ell_{11} & \\mathbf{b}^T \\\\ \\mathbf{0}_{n-1} & \\mathbf{L}_{22}^T\n",
    "\\end{bmatrix}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "entails\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\ta_{11} &=& \\ell_{11}^2 \\\\\n",
    "\t\\mathbf{a} &=& \\ell_{11} \\mathbf{b}\t\\\\\n",
    "\t\\mathbf{A}_{22} &=&   \\mathbf{b} \\mathbf{b}^T + \\mathbf{L}_{22} \\mathbf{L}_{22}^T\n",
    "    .\n",
    "\\end{eqnarray*}\n",
    "$$  \n",
    "Since $a_{11}>0$ (why?), $\\ell_{11}=\\sqrt{a_{11}}$ and $\\mathbf{b}=a_{11}^{-1/2}\\mathbf{a}$ are uniquely determined. \n",
    "$\\mathbf{A}_{22} - \\mathbf{b} \\mathbf{b}^T = \\mathbf{A}_{22} - a_{11}^{-1} \\mathbf{a} \\mathbf{a}^T$ is positive definite of size $(n-1)\\times(n-1)$ because $\\mathbf{A}_{22}$ is positive definite (why? homework). By induction hypothesis, lower triangular $\\mathbf{L}_{22}$ such that $\\mathbf{A}_{22} - \\mathbf{b} \\mathbf{b}^T = \\mathbf{L}_{22}^T\\mathbf{L}_{22}$ exists and is unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This proof is constructive and completely specifies the algorithm: \n",
    "```Julia\n",
    "for k=1:n\n",
    "    for j=k+1:n\n",
    "        a_jk_div_a_kk = A[j, k] / A[k, k] \n",
    "        for i=j:n\n",
    "            A[i, j] -= A[i, k] * a_jk_div_a_kk    # L_22\n",
    "        end\n",
    "    end\n",
    "    sqrt_akk = sqrt(A[k, k])\n",
    "    for i=k:n\n",
    "        A[i, k] /= sqrt_akk    # l_11 and b\n",
    "    end\n",
    "end\n",
    "```\n",
    "\n",
    "<img src=\"http://www.netlib.org/utk/papers/factor/_25826_figure440.gif\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "Source: <http://www.netlib.org>\n",
    "\n",
    "* Computational cost: \n",
    "$$\n",
    "\\frac{1}{2} [2(n-1)^2 + 2(n-2)^2 + \\cdots + 2 \\cdot 1^2] \\approx \\frac{1}{3} n^3 \\quad \\text{flops}\n",
    "$$ \n",
    "plus $n$ square roots. **Half** the cost of LU decomposition.\n",
    "\n",
    "* In general Cholesky decomposition is very stable. Failure of the decomposition simply means $\\mathbf{A}$ is not positive definite. **It is an efficient way to test positive definiteness.**\n",
    "\n",
    "\n",
    "## Pivoting\n",
    "\n",
    "* Pivoting is only needed if you want the Cholesky factor of a positive *semidefinite* matrix.\n",
    "\n",
    "* When $\\mathbf{A}$ does not have full rank, e.g., $\\mathbf{X}^T \\mathbf{X}$ with a non-full column rank $\\mathbf{X}$, we encounter $a_{kk} = 0$ during the procedure.\n",
    "\n",
    "* **Symmetric pivoting**. At each stage $k$, we permute both row and column such that $\\max_{k \\le i \\le n} a_{ii}$ becomes the pivot. If we encounter $\\max_{k \\le i \\le n} a_{ii} = 0$, then $\\mathbf{A}[k:n,k:n] = \\mathbf{0}$ (why?) and the algorithm terminates.\n",
    "\n",
    "* With symmetric pivoting: \n",
    "$$\n",
    "\\mathbf{P} \\mathbf{A} \\mathbf{P}^T = \\mathbf{L} \\mathbf{L}^T,\n",
    "$$\n",
    "where $\\mathbf{P}$ is a permutation matrix and $\\mathbf{L} \\in \\mathbb{R}^{n \\times r}$, $r = \\text{rank}(\\mathbf{A})$.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "* LAPACK functions: [`?potrf`](http://www.netlib.org/lapack/explore-html/d1/d7a/group__double_p_ocomputational_ga2f55f604a6003d03b5cd4a0adcfb74d6.html#ga2f55f604a6003d03b5cd4a0adcfb74d6) (without pivoting), [`?pstrf`](http://www.netlib.org/lapack/explore-html/da/dba/group__double_o_t_h_e_rcomputational_ga31cdc13a7f4ad687f4aefebff870e1cc.html#ga31cdc13a7f4ad687f4aefebff870e1cc) (with pivoting).\n",
    "\n",
    "* Julia functions: [`cholesky`](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.cholesky), [`cholesky!`](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.cholesky!), or call LAPACK wrapper functions [`potrf!`](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.LAPACK.potrf!) and [`pstrf!`](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.LAPACK.pstrf!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: positive definite matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "A = Float64.([4 12 -16; 12 37 -43; -16 -43 98])  # check out this is pd!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cholesky without pivoting\n",
    "Achol = cholesky(Symmetric(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeof(Achol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fieldnames(typeof(Achol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the lower triangular Cholesky factor\n",
    "Achol.L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the upper triangular Cholesky factor\n",
    "Achol.U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [1.0; 2.0; 3.0]\n",
    "A \\ b # this does LU; wasteful!; 2/3 n^3 + 2n^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Achol \\ b # two triangular solves; only 2n^2 flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det(A) # this actually does LU; wasteful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det(Achol) # cheap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv(A) # this does LU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv(Achol) # 2n triangular solves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: positive *semi*definite matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "\n",
    "Random.seed!(123) # seed\n",
    "A = randn(5, 3)\n",
    "A = A * transpose(A) # A has rank 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Achol = cholesky(A, Val(true)) # 2nd argument requests pivoting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Achol = cholesky(A, Val(true), check=false) # turn off checking pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank(Achol) # determine rank from Cholesky factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank(A) # determine rank from SVD, which is more numerically stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Achol.L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Achol.U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Achol.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Achol.P   # this returns P'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P A P' = L U\n",
    "norm(transpose(Achol.P) * A * Achol.P - Achol.L * Achol.U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications\n",
    "\n",
    "* **No inversion** mentality: Whenever we see matrix inverse, we should think in terms of solving linear equations. If the matrix is positive (semi)definite, use Cholesky decomposition, which is twice cheaper than LU decomposition.\n",
    "\n",
    "### Multivariate normal density \n",
    "\n",
    "Multivariate normal density $\\mathcal{N}(0, \\Sigma)$, where $\\Sigma$ is $n\\times n$ positive definite, is\n",
    "$$\n",
    "    \\frac{1}{(2\\pi)^{n/2}\\det(\\Sigma)^{1/2}}\\exp\\left(-\\frac{1}{2}\\mathbf{y}^T\\Sigma^{-1}\\mathbf{y}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence the log likelihood is\n",
    "$$\n",
    "- \\frac{n}{2} \\log (2\\pi) - \\frac{1}{2} \\log \\det \\Sigma - \\frac{1}{2} \\mathbf{y}^T \\Sigma^{-1} \\mathbf{y}.\n",
    "$$\n",
    "\n",
    "* Method 1: \n",
    "    1. compute explicit inverse $\\Sigma^{-1}$ ($2n^3$ flops), \n",
    "    2. compute quadratic form ($2n^2 + 2n$ flops), \n",
    "    3. compute determinant ($2n^3/3$ flops).\n",
    "    \n",
    "* Method 2: \n",
    "    1. Cholesky decomposition $\\Sigma = \\mathbf{L} \\mathbf{L}^T$ ($n^3/3$ flops), \n",
    "    2. Solve $\\mathbf{L} \\mathbf{x} = \\mathbf{y}$ by forward substitutions ($n^2$ flops), \n",
    "    3. compute quadratic form $\\mathbf{x}^T \\mathbf{x}$ ($2n$ flops), and (d) compute determinant from Cholesky factor ($n$ flops).  \n",
    "\n",
    "**Which method is better?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word-by-word transcription of mathematical expression\n",
    "function logpdf_mvn_1(y::Vector, Σ::Matrix)\n",
    "    n = length(y)\n",
    "    - (n//2) * log(2π) - (1//2) * logdet(Σ) - (1//2) * transpose(y) * inv(Σ) * y\n",
    "end\n",
    "\n",
    "# you learn't why you should avoid inversion\n",
    "function logpdf_mvn_2(y::Vector, Σ::Matrix)\n",
    "    n = length(y)\n",
    "    Σchol = cholesky(Symmetric(Σ))\n",
    "    - (n//2) * log(2π) - (1//2) * logdet(Σchol) - (1//2) * dot(y, Σchol \\ y)\n",
    "end\n",
    "\n",
    "# this is for the efficiency-concerned  \n",
    "function logpdf_mvn_3(y::Vector, Σ::Matrix)\n",
    "    n = length(y)\n",
    "    Σchol = cholesky(Symmetric(Σ))\n",
    "    - (n//2) * log(2π) - sum(log.(diag(Σchol.L))) - (1//2) * sum(abs2, Σchol.L \\ y)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools, Distributions, Random\n",
    "\n",
    "Random.seed!(123) # seed\n",
    "\n",
    "n = 1000\n",
    "# a pd matrix\n",
    "Σ = convert(Matrix{Float64}, Symmetric([i * (n - j + 1) for i in 1:n, j in 1:n]))\n",
    "y = rand(MvNormal(Σ)) # one random sample from N(0, Σ)\n",
    "\n",
    "# at least they give same answer\n",
    "@show logpdf_mvn_1(y, Σ)\n",
    "@show logpdf_mvn_2(y, Σ)\n",
    "@show logpdf_mvn_3(y, Σ);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark logpdf_mvn_1($y, $Σ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark logpdf_mvn_2($y, $Σ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark logpdf_mvn_3($y, $Σ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To evaluate same multivariate normal density at many observations $y_1, y_2, \\ldots$, we pre-compute the Cholesky decomposition ($n^3/3$ flops), then each evaluation costs $n^2$ flops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression by Cholesky\n",
    "\n",
    "* Cholesky decomposition is **one** approach to solve linear regression. Assume $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ and $\\mathbf{y} \\in \\mathbb{R}^n$.  \n",
    "    - Compute $\\mathbf{X}^T \\mathbf{X}$: $np^2$ flops  \n",
    "    - Compute $\\mathbf{X}^T \\mathbf{y}$: $2np$ flops  \n",
    "    - Cholesky decomposition of $\\mathbf{X}^T \\mathbf{X}$: $\\frac{1}{3} p^3$ flops  \n",
    "    - Solve normal equation $\\mathbf{X}^T \\mathbf{X} \\beta = \\mathbf{X}^T \\mathbf{y}$: $2p^2$ flops  \n",
    "    - If need standard errors, another $(4/3)p^3$ flops\n",
    "\n",
    "Total computational cost is $np^2 + (1/3) p^3$ (without s.e.) or $np^2 + (5/3) p^3$ (with s.e.) flops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "* Section 7.7 of [Numerical Analysis for Statisticians](https://link.springer.com/book/10.1007/978-1-4419-5945-4) of Kenneth Lange (2010).\n",
    "\n",
    "* Section II.5.3 of [Computational Statistics](https://link.springer.com/book/10.1007%2F978-0-387-98144-4) by James Gentle (2010).\n",
    "\n",
    "* Section 4.2 of [Matrix Computation](https://www.amazon.com/Computations-Hopkins-Studies-Mathematical-Sciences/dp/1421407949/ref=sr_1_1?keywords=matrix+computation+golub&qid=1567157884&s=gateway&sr=8-1) by Gene Golub and Charles Van Loan (2013)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgment\n",
    "\n",
    "Many parts of this lecture note is based on [Dr. Hua Zhou](http://hua-zhou.github.io)'s 2019 Spring Statistical Computing course notes available at <http://hua-zhou.github.io/teaching/biostatm280-2019spring/index.html>."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.4.0",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "118px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
