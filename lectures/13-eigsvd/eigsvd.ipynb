{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"../..\")\n",
    "Pkg.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenvalue decomposition and singular value decomposition\n",
    "\n",
    "* We already saw wide applications of QR decomposition in least squares problem and solving square and underdetermined system of linear equations. \n",
    "\n",
    "* EVD and SVD can be deemed as more thorough orthogonalization of a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVD review\n",
    "\n",
    "* **Eigenvalues** are defined as roots of the characteristic equation $\\det(\\lambda \\mathbf{I}_n - \\mathbf{A})=0$.\n",
    "    - $\\lambda$ can be *complex*.\n",
    "\n",
    "* If $\\lambda$ is an eigenvalue of $\\mathbf{A}$, then there exist non-zero $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n$ such that $\\mathbf{A} \\mathbf{x} = \\lambda \\mathbf{x}$ and $\\mathbf{y}^T \\mathbf{A} = \\lambda \\mathbf{y}^T$. The $\\mathbf{x}$ and $\\mathbf{y}$ are called the **right eigenvector** and **left eigenvector** of $\\mathbf{A}$ associated with the eigenvalue $\\lambda$.\n",
    "\n",
    "* $\\mathbf{A}$ is singular if and only if it has at least one 0 eigenvalue.\n",
    "\n",
    "* Eigenvectors associated with distinct eigenvalues are linearly independent.\n",
    "\n",
    "* Eigenvalues of an upper or lower triangular matrix are its diagonal entries: $\\lambda_i = a_{ii}$.\n",
    "\n",
    "* Eigenvalues of an idempotent matrix are either 0 or 1.\n",
    "\n",
    "* Eigenvalues of an orthogonal matrix have complex modulus 1.\n",
    "\n",
    "* In most statistical applications, we deal with eigenvalues/eigenvectors of symmetric matrices. \n",
    "The eigenvalues and eigenvectors of a real **symmetric** matrix are real.\n",
    "\n",
    "* Eigenvectors associated with distinct eigenvalues of a symmetric matrix are orthogonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Eigenvalue decomposition (EVD): $\\mathbf{A} = \\mathbf{X} \\Lambda \\mathbf{X}^{-1}$.\n",
    "    * $\\Lambda = \\text{diag}(\\lambda_1,\\ldots,\\lambda_n)$ collects the eigenvalues of $\\mathbf{A}$.\n",
    "    * Columns of $\\mathbf{X}$ are the eigenvectors.\n",
    "    \n",
    "* Not all matrices have EVD:\n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix} 2 & 1 & 0 \\\\ 0 & 2 & 1 \\\\ 0 & 0 & 2 \\end{bmatrix}\n",
    "$$\n",
    "has only a single independent eigenvector $\\mathbf{e}_1 = (1, 0, 0)^T$ associated with the multiple eigenvalue 2.\n",
    "\n",
    "* **Schur decomposition**: *every* square matrix $\\mathbf{A}$ can be decomposed $\\mathbf{A} = \\mathbf{Q}\\mathbf{T}\\mathbf{Q}^*$\n",
    "    - $\\mathbf{T}$ is upper triangular, with eigenvalues on the diagonal.\n",
    "    - $\\mathbf{Q}$ is unitary: $\\mathbf{Q}^*\\mathbf{Q} = \\mathbf{I}$.\n",
    "    - Columns of $\\mathbf{Q}$ form a sequence of nested invariant spaces with respect to $\\mathbf{A}$: let $\\mathbf{Q} = [\\mathbf{q}_1 | \\dotsb | \\mathbf{q}_n ]$. Then $\\text{span}(\\mathbf{q}_1, \\dotsc, \\mathbf{q}_k) = \\text{span}(\\mathbf{A}\\mathbf{q}_1, \\dotsc, \\mathbf{A}\\mathbf{q}_k)$.\n",
    "\n",
    "* **Eigenvalue decompostion of a symmetric matrix**: $\\mathbf{A} = \\mathbf{U} \\Lambda \\mathbf{U}^T$, where\n",
    "    * $\\Lambda = \\text{diag}(\\lambda_1,\\ldots,\\lambda_n)$\n",
    "    * columns of $\\mathbf{U}$ are the eigenvectors, which are (or can be chosen to be) mutually orthonormal. Thus $\\mathbf{U}$ is an orthogonal matrix.\n",
    "\n",
    "* A real symmetric matrix is positive semidefinite (positive definite) if and only if all eigenvalues are nonnegative (positive).\n",
    "\n",
    "* **Spectral radius** $\\rho(\\mathbf{A}) = \\max_i |\\lambda_i|$.\n",
    "\n",
    "* If $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ a square matrix (not required to be symmetric), then $\\text{tr}(\\mathbf{A}) = \\sum_i \\lambda_i$ and $\\det(\\mathbf{A}) = \\prod_i \\lambda_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD review\n",
    "\n",
    "* **Singular value decomposition (SVD)**: For a rectangular matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, let $p = \\min\\{m,n\\}$, then we have the SVD\n",
    "$$\n",
    "\\mathbf{A} = \\mathbf{U} \\Sigma \\mathbf{V}^T,\n",
    "$$\n",
    "where\n",
    "    * $\\mathbf{U} = (\\mathbf{u}_1,\\ldots,\\mathbf{u}_m) \\in \\mathbb{R}^{m \\times m}$ is orthogonal, i.e. $\\mathbf{U}^T\\mathbf{U} = \\mathbf{U}\\mathbf{U}^T = \\mathbf{I}_m$.\n",
    "    * $\\mathbf{V} = (\\mathbf{v}_1,\\ldots,\\mathbf{v}_n) \\in \\mathbb{R}^{n \\times n}$ is orthogonal, i.e. $\\mathbf{V}^T\\mathbf{V} = \\mathbf{V}\\mathbf{V}^T = \\mathbf{I}_n$.\n",
    "    * $\\Sigma = [\\text{diag}(\\sigma_1, \\ldots, \\sigma_p)~\\mathbf{0}]~\\text{or}~[\\text{diag}(\\sigma_1, \\ldots, \\sigma_p); \\mathbf{0}] \\in \\mathbb{R}^{m \\times n}$, $\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_p \\ge 0$.  \n",
    "$\\sigma_i$ are called the **singular values**, $\\mathbf{u}_i$ are the **left singular vectors**, and $\\mathbf{v}_i$ are the **right singular vectors**.\n",
    "\n",
    "* **Thin/skinny SVD**. Assume $m \\ge n$. $\\mathbf{A}$ can be factored as \n",
    "$$\n",
    "\\mathbf{A} = \\mathbf{U}_n \\Sigma_n \\mathbf{V}^T = \\sum_{i=1}^n \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T,\n",
    "$$ \n",
    "where \n",
    "    * $\\mathbf{U}_n \\in \\mathbb{R}^{m \\times n}$, $\\mathbf{U}_n^T \\mathbf{U}_n = \\mathbf{I}_n$\n",
    "    * $\\mathbf{V} \\in \\mathbb{R}^{n \\times n}$, $\\mathbf{V}^T \\mathbf{V} = \\mathbf{V} \\mathbf{V}^T \\mathbf{I}_n$\n",
    "    * $\\Sigma_n = \\text{diag}(\\sigma_1,\\ldots,\\sigma_n) \\in \\mathbb{R}^{n \\times n}$, $\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_n \\ge 0$\n",
    "    \n",
    "* Denote $\\sigma(\\mathbf{A})=(\\sigma_1,\\ldots,\\sigma_p)^T$. Then \n",
    "    * $r = \\text{rank}(\\mathbf{A}) = \\# \\text{ nonzero singular values} = \\|\\sigma(\\mathbf{A})\\|_0$  \n",
    "    * $\\mathbf{A} = \\mathbf{U}_r \\Sigma_r \\mathbf{V}_r^T = \\sum_{i=1}^r \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T$\n",
    "    * $\\|\\mathbf{A}\\|_{\\text{F}} = (\\sum_{i=1}^p \\sigma_i^2)^{1/2} = \\|\\sigma(\\mathbf{A})\\|_2$\n",
    "    * $\\|\\mathbf{A}\\|_2 = \\sigma_1 = \\|\\sigma(\\mathbf{A})\\|_\\infty$\n",
    "\n",
    "* Assume $\\text{rank}(\\mathbf{A}) = r$ and partition \n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\mathbf{U} &=& [\\mathbf{U}_r, \\tilde{\\mathbf{U}}_r] \\in \\mathbb{R}^{m \\times m} \\\\\n",
    "\\mathbf{V} &=& [\\mathbf{V}_r, \\tilde{\\mathbf{V}}_r] \\in \\mathbb{R}^{n \\times n}.\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "Then\n",
    "    * ${\\cal R}(\\mathbf{A}) = {\\cal R}(\\mathbf{U}_r)$, ${\\cal N}(\\mathbf{A}^T) = {\\cal R}(\\tilde{\\mathbf{U}}_r)={\\cal R}(\\mathbf{A})^\\perp$\n",
    "    * ${\\cal N}(\\mathbf{A}) = {\\cal R}(\\tilde{\\mathbf{V}}_r)$, ${\\cal R}(\\mathbf{A}^T) = {\\cal R}(\\mathbf{V}_r)$\n",
    "    * $\\mathbf{U}_r \\mathbf{U}_r^T$ is the orthogonal projection onto ${\\cal R}(\\mathbf{A})$\n",
    "    * $\\tilde{\\mathbf{U}}_r \\tilde{\\mathbf{U}}_r^T$ is the orthogonal projection onto ${\\cal N}(\\mathbf{A}^T)$\n",
    "    * $\\mathbf{V}_r \\mathbf{V}_r^T$ is the orthogonal projection onto ${\\cal R}(\\mathbf{A}^T)$\n",
    "    * $\\tilde{\\mathbf{V}}_r \\tilde{\\mathbf{V}}_r^T$ is the orthogonal projection onto ${\\cal N}(\\mathbf{A})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation between EVD and SVD\n",
    "\n",
    "* Using thin SVD,\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\mathbf{A}^T \\mathbf{A} &=& \\mathbf{V} \\Sigma \\mathbf{U}^T \\mathbf{U} \\Sigma \\mathbf{V}^T = \\mathbf{V} \\Sigma^2 \\mathbf{V}^T \\\\\n",
    "\t\\mathbf{A} \\mathbf{A}^T &=& \\mathbf{U} \\Sigma \\mathbf{V}^T \\mathbf{V} \\Sigma \\mathbf{U}^T = \\mathbf{U} \\Sigma^2 \\mathbf{U}^T.\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "In principle we can obtain singular triplets of $\\mathbf{A}$ by doing two EVDs.\n",
    "\n",
    "* In fact, using thin SVD,\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\begin{bmatrix} \\mathbf{0}_{n \\times n} & \\mathbf{A}^T \\\\ \\mathbf{A} & \\mathbf{0}_{m \\times m} \\end{bmatrix} \n",
    "    = \\frac{1}{\\sqrt 2} \n",
    "    \\begin{bmatrix} \\mathbf{V} & \\mathbf{V} \\\\ \\mathbf{U} & -\\mathbf{U} \\end{bmatrix} \n",
    "    \\begin{bmatrix} \\Sigma & \\mathbf{0}_{n \\times n} \\\\ \\mathbf{0}_{n \\times n} & - \\Sigma \\end{bmatrix} \n",
    "    \\frac{1}{\\sqrt 2} \n",
    "    \\begin{bmatrix} \\mathbf{V}^T & \\mathbf{U}^T \\\\ \\mathbf{V}^T & - \\mathbf{U}^T \\end{bmatrix}.\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "Hence any symmetric EVD solver can produce the SVD of a matrix $\\mathbf{A}$ without forming $\\mathbf{A} \\mathbf{A}^T$ or $\\mathbf{A}^T \\mathbf{A}$.\n",
    "\n",
    "* Yet another relation: If the EVD of a real *symmetric* matrix is $\\mathbf{A} = \\mathbf{W} \\Lambda \\mathbf{W}^T = \\mathbf{W} \\text{diag}(\\lambda_1, \\ldots, \\lambda_n) \\mathbf{W}^T$, then\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\mathbf{A} = \\mathbf{W} \\Lambda \\mathbf{W}^T = \\mathbf{W} \n",
    "    \\begin{bmatrix} \n",
    "\t|\\lambda_1| & & \\\\\n",
    "\t& \\ddots & \\\\\n",
    "\t& & |\\lambda_n|\n",
    "\t\\end{bmatrix} \n",
    "    \\begin{bmatrix} \n",
    "\t\\text{sgn}(\\lambda_1) & & \\\\\n",
    "\t& \\ddots & \\\\\n",
    "\t& & \\text{sgn}(\\lambda_n)\n",
    "\t\\end{bmatrix} \\mathbf{W}^T\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "is the SVD of $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of EVD and SVD\n",
    "\n",
    "\n",
    "### Principal components analysis (PCA)\n",
    "\n",
    "$\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ is a centered data matrix. Perform SVD $\\mathbf{X} = \\mathbf{U} \\Sigma \\mathbf{V}^T$ or equivalently eigendecomposition $\\mathbf{X}^T \\mathbf{X} = \\mathbf{V} \\Sigma^2 \\mathbf{V}^T$. The linear combinations $\\tilde{\\mathbf{x}}_i = \\mathbf{X} \\mathbf{v}_i$ are the **principal components** (PC) and have variance $\\sigma_i^2$.\n",
    "\n",
    "* Dimension reduction: reduce dimensionality $p$ to $q \\ll p$. Use top PCs $\\tilde{\\mathbf{x}}_1, \\ldots, \\tilde{\\mathbf{x}}_q$ in visualization and downstream analysis.\n",
    "\n",
    "<img src=\"./novembre-nature.jpg\" width=\"400\" align=\"center\"/>\n",
    "\n",
    "Above picture is from the article [Genes mirror geography within Europe](http://www.nature.com/nature/journal/v456/n7218/full/nature07331.html) by Novembre et al (2008) published in _Nature_.  \n",
    "\n",
    "* Use PCs to adjust for confounding - a serious issue in association studies in large data sets.\n",
    "    * Use of PCA to adjust for confounding in modern genetic studies is proposed in the paper [Principal components analysis corrects for stratification in genome-wide association studies](http://www.nature.com/ng/journal/v38/n8/full/ng1847.html) by Price et al (2006) published in _Nature Genetics_. It has been cited 5,550 times as of Oct 12, 2020.\n",
    "    \n",
    "### Low rank approximation\n",
    "\n",
    "For example, image/data compression. Find a low rank approximation of data matrix $\\mathbf{X}$.  \n",
    "**Eckart-Young theorem**: \n",
    "$$\n",
    "\\min_{\\text{rank}(\\mathbf{Y})=r} \\|\\mathbf{X} - \\mathbf{Y} \\|_{\\text{F}}^2\n",
    "$$\n",
    "is achieved by $\\mathbf{Y} = \\sum_{i=1}^r \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T$ with optimal value $\\sum_{i=r}^{p} \\sigma_i^2$, where $(\\sigma_i, \\mathbf{u}_i, \\mathbf{v}_i)$ are singular values and vectors of $\\mathbf{X}$.\n",
    "\n",
    "* [Gene Golub](https://en.wikipedia.org/wiki/Gene_H._Golub)'s $897 \\times 598$ picture requires $3 \\times 897 \\times 598 \\times 8 = 12,873,744$ bytes (3 RGB channels).  \n",
    "\n",
    "<img src=\"https://www.mathworks.com/content/mathworks/www/en/company/newsletters/articles/professor-svd/_jcr_content/mainParsys/columns/3/image_7.img.jpg/1490211663024.jpg\" width=\"150\" align=\"center\"/>\n",
    "\n",
    "* Rank 50 approximation requires $3 \\times 50 \\times (897 + 598) \\times 8 = 1,794,000$ bytes. \n",
    "\n",
    "<img src=\"https://www.mathworks.com/content/mathworks/www/en/company/newsletters/articles/professor-svd/_jcr_content/mainParsys/columns/2/image_6.img.jpg/1490211662986.jpg\" width=\"150\" align=\"center\"/>\n",
    "\n",
    "* Rank 12 approximation requires $12 \\times (2691+598) \\times 8 = 430,560$ bytes.\n",
    "\n",
    "<img src=\"https://www.mathworks.com/content/mathworks/www/en/company/newsletters/articles/professor-svd/_jcr_content/mainParsys/columns/1/image_5.img.jpg/1490211662955.jpg\" width=\"150\" align=\"center\"/>\n",
    "\n",
    "Source: [Professor SVD](https://www.mathworks.com/company/newsletters/articles/professor-svd.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moore-Penrose (MP) inverse\n",
    "\n",
    "Using thin SVD, \n",
    "$$\n",
    "\\mathbf{A}^{\\dagger} = \\mathbf{V} \\Sigma^{\\dagger} \\mathbf{U}^T,\n",
    "$$\n",
    "where $\\Sigma^{\\dagger} = \\text{diag}(\\sigma_1^{-1}, \\ldots, \\sigma_r^{-1}, 0, \\ldots, 0)$, $r= \\text{rank}(\\mathbf{A})$. This is how the [`pinv`](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.pinv) function is implemented in Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random, LinearAlgebra\n",
    "\n",
    "Random.seed!(280)\n",
    "X = randn(5, 3)\n",
    "pinv(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculation of Moore-Penrose inverse by SVD\n",
    "@which pinv(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares via SVD\n",
    "\n",
    "When $\\mathbf{X}$ does not have full column rank, the Moore-Penrose pseudoinverse gives the minimum $\\ell_2$ norm solution to OLS. To see this, let $\\mathbf{X} = \\sum_{i=1}^{\\min(n,p)}\\sigma_i\\mathbf{u}_i\\mathbf{v}_i^T$ (full SVD) , $r=\\text{rank}(\\mathbf{X})$ and $\\mathbf{\\beta} = \\sum_{j=1}^p \\alpha_j\\mathbf{v}_j$.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\|\\mathbf{y} - \\mathbf{X}\\beta\\|_2^2 &= \\|\\mathbf{y} - \\mathbf{U}\\Sigma_p\\mathbf{V}^T\\beta\\|_2^2 \\\\\n",
    "    &= \\|\\sum_{i=1}^n(\\mathbf{u}_i^T\\mathbf{y})\\mathbf{u}_i - \\sum_{i=1}^{\\min(n,p)}\\sigma_i\\alpha_i\\mathbf{u}_i\\|_2^2\n",
    "    = \\|\\sum_{i=1}^n(\\mathbf{u}_i^T\\mathbf{y} - \\sigma_i\\alpha_i)\\mathbf{u}_i\\|_2^2\n",
    "    = \\sum_{i=1}^n(\\mathbf{u}_i^T\\mathbf{y} - \\sigma_i\\alpha_i)^2\n",
    "    \\\\\n",
    "    &= \\sum_{i=1}^r(\\mathbf{u}_i^T\\mathbf{y} - \\sigma_i\\alpha_i)^2 + \\sum_{i=r+1}^n(\\mathbf{u}_i^T\\mathbf{y})^2\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "  Thus $\\hat{\\alpha}_j = \\frac{1}{\\sigma_j}\\mathbf{u}_j^T\\mathbf{y}$ for $j=1,\\dotsc, r$ but is arbitrary for $j>r$. The minimum $\\ell_2$ norm solution is given by setting $\\hat{\\alpha}_j=0$ for $j>r$, resulting in\n",
    "\n",
    "$$\n",
    "    \\hat\\beta = \\sum_{j=1}^r\\frac{\\mathbf{u}_j^T\\mathbf{y}}{\\sigma_j}\\mathbf{v}_j = \\mathbf{X}^{\\dagger}\\mathbf{y}\n",
    "$$\n",
    "  \n",
    "  and\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\widehat{\\mathbf{y}} &=& \\mathbf{X} \\widehat \\beta = \\mathbf{U}_r \\mathbf{U}_r^T \\mathbf{y}.\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "  In general, SVD is more expensive than other approaches (Cholesky, QR). In some applications, SVD is computed for other purposes then we get least squares solution for free."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression\n",
    "\n",
    "* In ridge regression, we minimize\n",
    "$$\n",
    "\t\\|\\mathbf{y} - \\mathbf{X} \\beta\\|_2^2 + \\lambda \\|\\beta\\|_2^2,\n",
    "$$\n",
    "where $\\lambda$ is a tuning parameter.\n",
    "\n",
    "* Ridge regression by augmented linear regression. Ridge regression problem is equivalent to\n",
    "$$\n",
    "\t\\left\\| \\begin{pmatrix} \\mathbf{y} \\\\ \\mathbf{0}_p \\end{pmatrix} - \\begin{pmatrix}\n",
    "\t\\mathbf{X} \\\\ \\sqrt \\lambda \\mathbf{I}_p\n",
    "\t\\end{pmatrix} \\beta \\right\\|_2^2.\n",
    "$$\n",
    "Therefore any methods for linear regression (e.g., LSQR) can be applied.\n",
    "\n",
    "* Ridge regression by method of normal equation. The normal equation for the ridge problem is\n",
    "$$\n",
    "\t(\\mathbf{X}^T \\mathbf{X} + \\lambda \\mathbf{I}_p) \\beta = \\mathbf{X}^T \\mathbf{y}.\n",
    "$$\n",
    "Therefore Cholesky decomposition can be used.\n",
    "\n",
    "* Ridge regression by SVD. If we obtain the (thin) SVD of $\\mathbf{X}$\n",
    "$$\n",
    "\t\\mathbf{X} = \\mathbf{U} \\Sigma_{p} \\mathbf{V}^T,\n",
    "$$\n",
    "then the normal equation reads\n",
    "$$\n",
    "\t\\mathbf{V}^T(\\Sigma^2 + \\lambda \\mathbf{I}_p) \\mathbf{V}^T \\beta = \\mathbf{V}^T\\Sigma \\mathbf{U}^T \\mathbf{y}.\n",
    "$$\n",
    "\n",
    "  We get\n",
    "$$\n",
    "\t\\widehat \\beta (\\lambda) = \\mathbf{V} (\\Sigma^2 + \\lambda \\mathbf{I}_p)^{-1}\\Sigma \\mathbf{U}^T \\mathbf{y} =\n",
    "    \\sum_{i=1}^p \\frac{\\sigma_i \\mathbf{u}_i^T \\mathbf{y}}{\\sigma_i^2 + \\lambda} \\mathbf{v}_i = \\sum_{i=1}^r \\frac{\\sigma_i \\mathbf{u}_i^T \\mathbf{y}}{\\sigma_i^2 + \\lambda} \\mathbf{v}_i, \\quad r = \\text{rank}(\\mathbf{X}).\n",
    "$$\n",
    "It is clear that \n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\lim_{\\lambda \\to 0} \\widehat \\beta (\\lambda) = \\mathbf{X}^{\\dagger}\\mathbf{y},\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "the minimum $\\ell_2$ norm solution,\n",
    "and $\\|\\widehat \\beta (\\lambda)\\|_2$ is monotone decreasing as $\\lambda$ increases.\n",
    "\n",
    "* Only one SVD is needed for all $\\lambda$ (!), in contrast to the method of augmented linear regression or Cholesky."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms for EVD\n",
    "\n",
    "> **Any EVD solver must be iterative.**\n",
    "\n",
    "* Abel (1824) showed that the real root of a polynomial of degree 5 or more with rational coefficients cannot be written using any expression invloving rational numbers, addtion, subtraction, multiplication, division, and $k$th roots.\n",
    "\n",
    "* This means that no algorithm can produce the exact roots of the characteristic polynomial of a matrix, i.e., eigenvalues, in a finite number of steps.\n",
    "\n",
    "* \"Direct\" EVD solver refers to a solver that reduces to a general matrix to a special form in finite flops, and then apply iterative methods that converges to EVD.\n",
    "\n",
    "* For ease of exposition, we restrict attention to real symmetric matrices, i.e., we assume $\\mathbf{A}=\\mathbf{A}^T \\in \\mathbb{R}^{n \\times n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One eigen-pair: power iteration\n",
    "\n",
    "* Iterates according to\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\mathbf{x}^{(t)} &\\gets& \\frac{1}{\\|\\mathbf{A} \\mathbf{x}^{(t-1)}\\|_2} \\mathbf{A} \\mathbf{x}^{(t-1)}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "from an initial guess $\\mathbf{x}^{(0)}$ of *unit norm*.\n",
    "\n",
    "* Suppose we arrange $|\\lambda_1| > |\\lambda_2| \\ge \\cdots \\ge |\\lambda_n|$ (the first inequality strict) with corresponding eigenvectors $\\mathbf{u}_i$, and expand $\\mathbf{x}^{(0)} = c_1 \\mathbf{u}_1 + \\cdots + c_n \\mathbf{u}_n$, then\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\mathbf{x}^{(t)} &=& \\frac{\\left( \\sum_i \\lambda_i^t \\mathbf{u}_i \\mathbf{u}_i^T \\right) \\left( \\sum_i c_i \\mathbf{u}_i \\right)}{\\|\\left( \\sum_i \\lambda_i^t \\mathbf{u}_i \\mathbf{u}_i^T \\right) \\left( \\sum_i c_i \\mathbf{u}_i \\right)\\|_2} \\\\\n",
    "\t&=& \\frac{\\sum_i c_i \\lambda_i^t \\mathbf{u}_i}{\\|\\sum_i c_i \\lambda_i^t \\mathbf{u}_i\\|_2}\t\\\\\n",
    "\t&=& \\frac{c_1 \\mathbf{u}_1 + c_2 (\\lambda_2/\\lambda_1)^t \\mathbf{u}_2 + \\cdots + c_n (\\lambda_n/\\lambda_1)^t \\mathbf{u}_n}{\\|c_1 \\mathbf{u}_1 + c_2 (\\lambda_2/\\lambda_1)^t \\mathbf{u}_2 + \\cdots + c_n (\\lambda_n/\\lambda_1)^t \\mathbf{u}_n\\|_2} \\left( \\frac{\\lambda_1}{|\\lambda_1|} \\right)^t.\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "Thus $\\mathbf{x}^{(t)}/\\left( \\frac{\\lambda_1}{|\\lambda_1|} \\right)^t \\to \\frac{c_1}{|c_1|}\\mathbf{u}_1$ as $t \\to \\infty$. The convergence rate is $|\\lambda_2|/|\\lambda_1|$.\n",
    "\n",
    "* $\\lambda_1^{(t)} = \\mathbf{x}^{(t)T} \\mathbf{A} \\mathbf{x}^{(t)}$ converges to $\\lambda_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse iteration\n",
    "\n",
    "* Apply power iteration to $\\mathbf{A}^{-1}$ to find the eigen-pair of smallest absolute value.\n",
    "\n",
    "* (May pre-compute LU or Cholesky of $\\mathbf{A}$).\n",
    "\n",
    "#### Shifted inverse iteration\n",
    "\n",
    "0. Initialize $\\mathbf{v}^{(0)} \\in \\mathbb{R}^{n}$ with $\\|\\mathbf{v}^{(0)}\\|_2=1$.\n",
    "0. For $t=1,2,\\ldots$, \n",
    "    1. Solve $(\\mathbf{A} - \\mu\\mathbf{I})\\mathbf{w} = \\mathbf{v}^{(t-1)}$ for $\\mathbf{w}$; (e.g., by LU or Cholesky)\n",
    "    2. $\\mathbf{v}^{(t)} \\gets \\mathbf{w}/\\|\\mathbf{w}\\|_2$\n",
    "    3. $\\lambda_t = (\\mathbf{v}^{(t)})^T\\mathbf{A}\\mathbf{v}^{(t)}$\n",
    "\n",
    "\n",
    "* Converges linearly to an eigenvalue close to the pre-specified $\\mu$.\n",
    "\n",
    "#### Rayleigh quotient iteration \n",
    "\n",
    "* Substitute the shift $\\mu$ in shifted inverse iteration with $\\mu_t$:\n",
    "\n",
    "0. Initialize $\\mathbf{v}^{(0)} \\in \\mathbb{R}^{n}$ with $\\|\\mathbf{v}^{(0)}\\|_2=1$.\n",
    "0. $\\mu^{(0)} = (\\mathbf{v}^{(0)})^T\\mathbf{A}\\mathbf{v}^{(0)}$\n",
    "0. For $t=1,2,\\ldots$, \n",
    "    1. Solve $(\\mathbf{A} - \\mu_{t-1}\\mathbf{I})\\mathbf{w} = \\mathbf{v}^{(t-1)}$ for $\\mathbf{w}$; (e.g., by LU or Cholesky)\n",
    "    2. $\\mathbf{v}^{(t)} \\gets \\mathbf{w}/\\|\\mathbf{w}\\|_2$\n",
    "    3. $\\mu^{(t)} = (\\mathbf{v}^{(t)})^T\\mathbf{A}\\mathbf{v}^{(t)}$\n",
    "\n",
    "* **Cubic** convergence to the eigen-pair closest to $\\mathbf{v}^{(0)}$.\n",
    "\n",
    "* Example: PageRank problem seeks top left eigenvector of transition matrix $\\mathbf{P}$ and costs $O(n)$ per iteration.\n",
    "\n",
    "* **Rayleigh quotient**:\n",
    "$$\n",
    "    r(\\mathbf{x}) = \\frac{\\mathbf{x}^T\\mathbf{A}\\mathbf{x}}{\\mathbf{x}^T\\mathbf{x}}\n",
    "$$\n",
    "\n",
    "  Note $r(\\mathbf{u}_i) = \\lambda_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top $r$ eigen-pairs: orthogonal iteration (a.k.a. simultaneous iteration)\n",
    "\n",
    "Generalization of power method to higher dimensional invariant subspace.\n",
    "\n",
    "#### Algorithm  \n",
    "\n",
    "0. Initialize $\\tilde{\\mathbf{Q}}^{(0)} \\in \\mathbb{R}^{n \\times r}$ with orthonormal columns\n",
    "0. For $t=1,2,\\ldots$, \n",
    "    1. $\\mathbf{Z}^{(t)} \\gets \\mathbf{A} \\tilde{\\mathbf{Q}}^{(t-1)}$   ($2n^2r$ flops)\n",
    "    2. $\\tilde{\\mathbf{Q}}^{(t)} \\tilde{\\mathbf{R}}^{(t)} \\gets \\mathbf{Z}^{(t)}$ (QR decomposition, $nr^2 - r^3/3$ flops)\n",
    "\n",
    "#### Motivation\n",
    "\n",
    "* Rewrite the power method: \n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "    \\mathbf{z}^{(t)} &\\gets& \\mathbf{A} \\mathbf{x}^{(t-1)} \\\\\n",
    "\t\\mathbf{x}^{(t)}\\|\\mathbf{z}^{(t)}\\|_2 &\\gets& \\mathbf{z}^{(t)}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "where $\\|\\mathbf{x}^{(t)}\\|_2=1$. The second step is the first iteration of the Gram-Schmidt QR.\n",
    "\n",
    "#### Convergence\n",
    "\n",
    "* $\\mathbf{Z}^{(t)}$ converges to the *eigenspace* of the largest $r$ eigenvalues if they are real and separated from remaining spectrum, i.e., $|\\lambda_1| > \\dotsb > |\\lambda_r| > |\\lambda_{r+1}| \\ge \\dotsb |\\lambda_n|$. The convergence rate is $\\max_{k=1,\\dotsc,r}|\\lambda_{k+1}|/|\\lambda_k|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full EVD: QR iteration (impractical)\n",
    "\n",
    "#### Algorithm  \n",
    "\n",
    "0. $\\mathbf{A}^{(0)} \\gets \\mathbf{A}$\n",
    "0. For $t=1,2,\\ldots$, \n",
    "    1. $\\mathbf{Q}^{(t)} \\mathbf{R}^{(t)} \\gets \\mathbf{A}^{(t-1)}$   (QR decomposition)\n",
    "    2. $\\mathbf{A}^{(t)} \\gets \\mathbf{R}^{(t)}\\mathbf{Q}^{(t)}$\n",
    "\n",
    "#### Explanation\n",
    "\n",
    "* It can be shown that QR iteration is equivalent to the orthogonal iteration starting with $\\mathbf{Q}^{(0)}=\\mathbf{I}$. Specifically,\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\tilde{\\mathbf{R}}^{(t)} &= \\mathbf{R}^{(t)} \\\\\n",
    "    \\tilde{\\mathbf{Q}}^{(t)} &= \\mathbf{Q}^{(1)} \\mathbf{Q}^{(2)} \\dotsb \\mathbf{Q}^{(t)} \\\\\n",
    "    \\mathbf{A}^{(t)} &= (\\tilde{\\mathbf{Q}}^{(t)})^T\\mathbf{A}\\tilde{\\mathbf{Q}}^{(t)}\n",
    "    .\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "*  Take $r=n$ in the orthogonal iteration. Then $\\tilde{\\mathbf{Q}}^{(t)}$ converges to the eigenspace $\\mathbf{U}$ of $\\mathbf{A}$. This implies that\n",
    "$$\n",
    "\t\\mathbf{A}^{(t)} = (\\tilde{\\mathbf{Q}}^{(t)})^T \\mathbf{A} \\tilde{\\mathbf{Q}}^{(t)}\n",
    "$$\n",
    "converges to a diagonal form $\\boldsymbol{\\Lambda} = \\text{diag}(\\lambda_1, \\ldots, \\lambda_n)$.\n",
    "\n",
    "* QR iteration is expensive in general: $O(n^3)$ per iteration and *linear* convergence rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full EVD: QR iteration with shifts\n",
    "\n",
    "#### Algorithm \n",
    "\n",
    "0. $\\mathbf{A}^{(0)} \\gets \\mathbf{A}$\n",
    "0. For $t=1,2,\\ldots$, \n",
    "    1. Pick a shift $\\sigma_t$  (e.g., $\\sigma_t = \\mathbf{A}_{nn}^{(t)}$)\n",
    "    2. $\\mathbf{Q}^{(t)} \\mathbf{R}^{(t)} \\gets \\mathbf{A}^{(t-1)} - \\sigma_t\\mathbf{I}$   (QR decomposition)\n",
    "    2. $\\mathbf{A}^{(t)} \\gets \\mathbf{R}^{(t)}\\mathbf{Q}^{(t)} + \\sigma_t\\mathbf{I}$\n",
    "\n",
    "#### Explanation\n",
    "\n",
    "* Can be shown to be equivalent to simultaneous shifted *inverse* iteration starting with\n",
    "$$\n",
    "\\mathbf{Q}^{(0)} = \\begin{bmatrix}  &  &  & 1 \\\\ & & 1 & \\\\ & \\dotsc & & \\\\ 1 & & & \\end{bmatrix}\n",
    "$$\n",
    "hence *cubic* convergence.\n",
    "    \n",
    "* Last column of $\\tilde{\\mathbf{Q}}^{(t)}$ is the result of applying $t$ steps of shifted inverse iteration to the elementary unit vector $\\mathbf{e}_n$.\n",
    "\n",
    "* Also connected to Rayleigh quotient iteration. Let $\\mathbf{q}_n^{(t)}$ be the last column of $\\tilde{\\mathbf{Q}}^{(t)}$. Then the Rayleigh quotient is\n",
    "$$\n",
    "    \\frac{(\\mathbf{q}_n^{(t)})^T\\mathbf{A}\\mathbf{q}_n^{(t)}}{(\\mathbf{q}_n^{(t)})^T\\mathbf{q}_n^{(t)}} \n",
    "    = (\\mathbf{q}_n^{(t)})^T\\mathbf{A}\\mathbf{q}_n^{(t)}\n",
    "    = \\mathbf{e}_n^T(\\tilde{\\mathbf{Q}}^{(t)})^T\\mathbf{A}\\tilde{\\mathbf{Q}}^{(t)}\\mathbf{e}_n \n",
    "    = \\mathbf{e}_n^T\\mathbf{A}^{(t)}\\mathbf{e}_n\n",
    "    = \\mathbf{A}_{nn}^{(t)}\n",
    "    .\n",
    "$$\n",
    "Thus $\\sigma_t = \\mathbf{A}_{nn}^{(t)}$ is a good choice for the shift (and free to compute!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical EVD algorithm\n",
    "\n",
    "* Implemented in LAPACK: used by Julia, Matlab, R.\n",
    "\n",
    "* Tridiagonalization (by Householder) + implicit shifted QR iteration on the tridiagonal matrix.\n",
    "   1. Direct phase: Householder tridiagonalization: $4n^3/3$ for eigenvalues only, $8n^3/3$ for both eigenvalues and eigenvectors. (Why can't we apply Householder to make it diagonal directly?)   \n",
    "$$\n",
    "\\mathbf{Q}_1^T\\mathbf{A} = \n",
    "\\begin{bmatrix} \n",
    "\\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} \\\\ \n",
    "0 & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times}  \\\\\n",
    "0 & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} \\\\\n",
    "0 & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} \\\\\n",
    "0 & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times}\n",
    "\\end{bmatrix} \n",
    "%\n",
    "\\rightarrow\n",
    "%\n",
    "\\mathbf{Q}_1^T\\mathbf{A}\\mathbf{Q}_1 = \n",
    "\\begin{bmatrix} \n",
    "\\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} \\\\ \n",
    "\\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times}  \\\\\n",
    "\\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} \\\\\n",
    "\\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} \\\\\n",
    "\\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times}\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "  vs.\n",
    "$$\n",
    "\\mathbf{Q}_1^T\\mathbf{A} = \n",
    "\\begin{bmatrix} \n",
    "\\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} \\\\ \n",
    "\\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times}  \\\\\n",
    "0 & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} \\\\\n",
    "0 & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} \\\\\n",
    "0 & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times}\n",
    "\\end{bmatrix} \n",
    "%\n",
    "\\rightarrow\n",
    "%\n",
    "\\mathbf{Q}_1^T\\mathbf{A}\\mathbf{Q}_1 = \n",
    "\\begin{bmatrix} \n",
    "\\boldsymbol{\\times} & \\boldsymbol{\\times} & 0 & 0 & 0 \\\\ \n",
    "\\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times}  \\\\\n",
    "0 & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} \\\\\n",
    "0 & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} \\\\\n",
    "0 & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times} & \\boldsymbol{\\times}\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "   2. Iterative phase: Built on QR iteration with shifts. Tridiangonal structure allows the QR decomposition cheap (\"implicit Q theorem\"). On average 1.3-1.6 QR iteration per eigenvalue, $\\sim 20n$ flops per iteration. So total operation count is about $30n^2$. Eigenvectors need an extra of about $6n^3$ flops.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| Phase                  | Eigenvalue   | Eigenvector |\n",
    "|------------------------|--------------|-------------|\n",
    "| Tridiagonal reduction  | $4n^3/3$     | + $4n^3/3$    |\n",
    "| implicit shifted QR    | $\\sim 30n^2$ | + $\\sim 6n^3$ |\n",
    "\n",
    "\n",
    "* Take-home message: **Don't request eigenvectors unless necessary**. Use [`eigvals`](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.eigvals) in Julia to request only eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsymmetric EVD\n",
    "\n",
    "* Reduction to the upper Hessenberg form (upper triangular + subdiagonal)\n",
    "\n",
    "* The **unsymmetric QR algorithm** obtains the real Schur decomposition of the Hessenberg matrix.\n",
    "    - Implicit Q theorem still applies, but more expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Julia functions: [`eigen`](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.eigen), [`eigen!`](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.eigen!), [`eigvals`](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.eigvals!), [`eigvecs`](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.eigvecs), [`eigmax`](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.eigmax), [`eigmin`](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.eigmin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(280)\n",
    "A = Symmetric(randn(5, 5), :U)\n",
    "Aeig = eigen(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenvalues\n",
    "Aeig.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenvectors\n",
    "Aeig.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inversion by EVD\n",
    "inv(Aeig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@which inv(Aeig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determinant by EVD\n",
    "det(Aeig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@which det(Aeig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvals(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigmax(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigmin(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't request eigenvectors unless needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools, Random, LinearAlgebra\n",
    "\n",
    "Random.seed!(280)\n",
    "n = 1000\n",
    "A = Symmetric(randn(n, n), :U)  # build from the upper triangular part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requesting eigenvalues only is cheaper\n",
    "@benchmark eigvals($A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requesting eigenvectors requires extra work\n",
    "@benchmark eigen($A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark eigvecs($A)  # same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm for singular value decomposition (SVD)\n",
    "\n",
    "Assume $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ and we seek the SVD $\\mathbf{A} = \\mathbf{U} \\mathbf{D} \\mathbf{V}^T$. \n",
    "\n",
    "\n",
    "### Golub-Kahan-Reinsch algorithm\n",
    "\n",
    "* Stage 1: Transform $\\mathbf{A}$ to an upper bidiagonal form $\\mathbf{B}$ (by Householder).  \n",
    "\n",
    "<img src=\"svd_bidiagonalization.png\" width=\"400\" align=\"center\"/>\n",
    "\n",
    "<img src=\"SVD_bidiagonalization_2.png\" width=\"400\" align=\"center\"/>\n",
    "    \n",
    "* Stage 2: Apply implicit shifted QR iteration to the tridiagonal matrix $\\mathbf{B}^T\\mathbf{B}$ *without explicitly forming it*. That is $\\mathbf{B}^{(t)}$ is *directly* formed from $\\mathbf{B}^{(t-1)}$.\n",
    "\n",
    "\n",
    "* $4m^2 n + 8mn^2 + 9n^3$ flops for a tall $(m \\ge n)$ matrix.\n",
    "\n",
    "### Example\n",
    "\n",
    "Julia functions: [`svd`](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.svd), [`svd!`](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.svd), [`svdvals`](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.svdvals), [`svdvals!`](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.svdvals!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(280)\n",
    "\n",
    "A = randn(5, 3)\n",
    "Asvd = svd(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asvd.U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vt is cheaper to extract than V\n",
    "Asvd.Vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asvd.V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asvd.S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Don't request singular vectors unless needed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(280)\n",
    "\n",
    "n, p = 1000, 500\n",
    "A = randn(n, p)\n",
    "@benchmark svdvals(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark svd(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lanczos/Arnoldi iterative method for top eigen-pairs\n",
    "\n",
    "* Recall the PageRank problem. We want to find the top left eigenvector of the transition matrix $\\mathbf{P}$ of size 10^9 by 10^9. Direct methods such as (unsymmetric) QR or SVD takes forever. Iterative methods such as power method is feasible. However power method may take a large number of iterations.\n",
    "\n",
    "### Krylov subspace methods\n",
    "* State-of-art iterative methods for obtaining the top eigenvalues/vectors or singular values/vectors of large **sparse** or **structured** matrices.\n",
    "\n",
    "#### Motivation\n",
    "\n",
    "* Recall the Rayleigh quotient\n",
    "$$\n",
    "    r(\\mathbf{x}) = \\frac{\\mathbf{x}^T\\mathbf{A}\\mathbf{x}}{\\mathbf{x}^T\\mathbf{x}}\n",
    "$$\n",
    "\n",
    "  Note $r(\\mathbf{u}_i) = \\lambda_i$.\n",
    "  \n",
    "* Suppose $\\mathbf{q}_1, \\dotsc, \\mathbf{q}_n$ form an orthonormal basis of $\\mathbb{R}^n$. Define\n",
    "$$\n",
    "    M_k \\triangleq \\max_{\\mathbf{u}\\in\\text{span}(\\mathbf{q}_1,\\dotsc,\\mathbf{q}_k)} r(\\mathbf{u}),\n",
    "    \\quad\n",
    "    \\text{and}\n",
    "    \\quad\n",
    "    m_k \\triangleq \\min_{\\mathbf{v}\\in\\text{span}(\\mathbf{q}_1,\\dotsc,\\mathbf{q}_k)} r(\\mathbf{v}).\n",
    "$$\n",
    "\n",
    "* Obviously\n",
    "$$\n",
    "\\begin{split}\n",
    "    M_1 &\\le M_2 \\le \\dotsb \\le M_n = \\lambda_1, \\\\\n",
    "    m_1 &\\ge m_2 \\ge \\dotsb \\ge m_n = \\lambda_n.\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "* The goal is to choose $\\mathbf{q}_1, \\mathbf{q}_2, \\dotsc$ so that $M_k \\approx \\lambda_1$.\n",
    "\n",
    "* Consider the gradient of the Rayleigh quotient:\n",
    "$$\n",
    "    \\nabla r(\\mathbf{x}) = \\frac{2}{\\mathbf{x}^T\\mathbf{x}}(\\mathbf{A}\\mathbf{x} - r(\\mathbf{x})\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "* Suppose $M_k = r(\\mathbf{u}_k)$ for some $\\mathbf{u}_k \\in \\text{span}(\\mathbf{q}_1,\\dotsc,\\mathbf{q}_k)$. If $\\nabla r(\\mathbf{u}_k) = 0$, we are done. Otherwise, it is reasonable to choose the next vector $\\mathbf{q}_{k+1}$ so that\n",
    "$$\n",
    "    \\nabla r(\\mathbf{u}_k) \\in \\text{span}(\\mathbf{q}_1,\\dotsc,\\mathbf{q}_k, \\mathbf{q}_{k+1})\n",
    "$$\n",
    "because $r(\\mathbf{x})$ increases steepest in this direction.\n",
    "\n",
    "* Since $\\nabla r(\\mathbf{u}_k) \\in \\text{span}(\\mathbf{x}, \\mathbf{A}\\mathbf{x})$, the above proposal is satisfied if\n",
    "$$\n",
    "    \\text{span}(\\mathbf{q}_1,\\dotsc,\\mathbf{q}_k) = \\text{span}(\\mathbf{q}_1, \\mathbf{A}\\mathbf{q}_1, \\dotsc, \\mathbf{A}^{k-1}\\mathbf{q}_1) = \\mathcal{K}^k(\\mathbf{A}, \\mathbf{q}_1).\n",
    "$$\n",
    "\n",
    "* Finding $\\mathbf{q}_1, \\mathbf{q}_2, \\dotsc$ amounts to computing orthonormal bases for $\\mathcal{K}^k(\\mathbf{A}, \\mathbf{q}_1)$ (Gram-Schmidt).\n",
    "\n",
    "* The eigenvalues of the $k\\times k$ matrix $\\mathbf{Q}_k^T\\mathbf{A}\\mathbf{Q}_k$ (either tridiagonal or upper Hessenberg), call the *Ritz values*, approximate the top $k$ eigenvalues of $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instances\n",
    "\n",
    "* **Lanczos method**: top eigen-pairs of a large _symmetric_ matrix.  (Use tridiagonal reduction $\\mathbf{Q}^T\\mathbf{A}\\mathbf{Q} = \\mathbf{T}$.)\n",
    "\n",
    "* **Arnoldi method**: top eigen-pairs of a large _asymmetric_ matrix. (Use Hessenberg reduction $\\mathbf{Q}^T\\mathbf{A}\\mathbf{Q} = \\mathbf{H}$.)\n",
    "\n",
    "* Both methods are also adapted to obtain top singular values/vectors of large sparse or structured matrices.\n",
    "\n",
    "* `eigs` and `svds` in Julia [Arpack.jl](https://github.com/JuliaLinearAlgebra/Arpack.jl) package and Matlab are wrappers of the ARPACK package, which implements Lanczos and Arnoldi methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MatrixDepot, SparseArrays\n",
    "\n",
    "# Download the Boeing/bcsstk34 matrix (sparse, pd, 588-by-588) from SuiteSparse collection\n",
    "# https://sparse.tamu.edu/Boeing\n",
    "A = matrixdepot(\"Boeing/bcsstk34\")\n",
    "# Change type of A from Symmetric{Float64,SparseMatrixCSC{Float64,Int64}} to SparseMatrixCSC\n",
    "A = sparse(A)\n",
    "@show typeof(A)\n",
    "Afull = Matrix(A)\n",
    "@show typeof(Afull)\n",
    "# actual sparsity level\n",
    "count(!iszero, A) / length(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using UnicodePlots\n",
    "spy(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 5 eigenvalues by LAPACK (QR algorithm)\n",
    "n = size(A, 1)\n",
    "@time eigvals(Symmetric(Afull), (n-4):n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Arpack\n",
    "# top 5 eigenvalues by iterative methods\n",
    "@time eigs(A; nev=5, ritzvec=false, which=:LM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark eigvals(Symmetric(Afull), (n-4):n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark eigs($A; nev=5, ritzvec=false, which=:LM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see >1000 fold speedup in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobi's method for symmetric EVD\n",
    "\n",
    "* One of the oldest ideas for computing eigenvalues, by [Jacobi](https://en.wikipedia.org/wiki/Carl_Gustav_Jacob_Jacobi) in 1845.\n",
    "\n",
    "Assume $\\mathbf{A} \\in \\mathbf{R}^{n \\times n}$ is symmetric and we seek the EVD of $\\mathbf{A} = \\mathbf{U} \\Lambda \\mathbf{U}^T$.\n",
    "\n",
    "* Idea: diagonalize a 2x2 matrix by similarity transform using an orthogonal matrix:\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix} c & -s \\\\ s & c \\end{bmatrix}\n",
    "    \\begin{bmatrix} a & d  \\\\ d & b \\end{bmatrix}\n",
    "    \\begin{bmatrix} c & s \\\\ -s & c \\end{bmatrix}\n",
    "    =\n",
    "    \\begin{bmatrix} * & 0 \\\\ 0 & * \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "  It can be easily seen that $c=\\cos\\theta$ and $s=\\sin\\theta$ satisifies \n",
    "$$\n",
    "    \\tan(2\\theta) = \\frac{2d}{b-a}\n",
    "$$\n",
    "if $a\\neq b$, otherwise $c=s=1/\\sqrt{2}$.\n",
    "\n",
    "* We can systematically reduce off-diagonal entries of $\\mathbf{A}$\n",
    "$$\n",
    "\\text{off}(\\mathbf{A}) = \\sum_i \\sum_{j \\ne i} a_{ij}^2\n",
    "$$\n",
    "by Jacobi/Givens rotations:\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\mathbf{J}(p,q,\\theta) = \\begin{pmatrix} \n",
    "\t1 & & 0 & & 0 & & 0 \\\\\n",
    "\t\\vdots & \\ddots & \\vdots & & \\vdots & & \\vdots \\\\\n",
    "\t0 & & \\cos(\\theta) & & \\sin(\\theta) & & 0 \\\\ \n",
    "\t\\vdots & & \\vdots & \\ddots & \\vdots & & \\vdots \\\\\n",
    "\t0 & & - \\sin(\\theta) & & \\cos(\\theta) & & 0 \\\\\n",
    "\t\\vdots & & \\vdots & & \\vdots & \\ddots & \\vdots \\\\\n",
    "\t0 & & 0 & & 0 & & 1 \\end{pmatrix}\n",
    "    .\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "$\\mathbf{J}(p,q,\\theta)$ is orthogonal.\n",
    "\n",
    "* Consider $\\mathbf{B} = \\mathbf{J}^T \\mathbf{A} \\mathbf{J}$. $\\mathbf{B}$ preserves the symmetry and eigenvalues of $\\mathbf{A}$. Taking \n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\begin{cases}\n",
    "\\tan (2\\theta) = 2a_{pq}/({a_{qq}-a_{pp}}) & \\text{if } a_{pp} \\ne a_{qq} \\\\\n",
    "\\theta = \\pi/4 & \\text{if } a_{pp}=a_{qq}\n",
    "\\end{cases}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "forces $b_{pq}=0$.\n",
    "\n",
    "* Since orthogonal transform preserves Frobenius norm, we have\n",
    "$$\n",
    "b_{pp}^2 + b_{qq}^2 = a_{pp}^2 + a_{qq}^2 + 2a_{pq}^2.\n",
    "$$ \n",
    "\n",
    "* Since $\\|\\mathbf{A}\\|_{\\text{F}} = \\|\\mathbf{B}\\|_{\\text{F}}$, \n",
    "$$\n",
    "\\begin{split}\n",
    "\\|\\mathbf{B}\\|_{\\text{F}}^2 &= \\text{off}(\\mathbf{B}) + \\|\\text{diag}(B)\\|_2^2  \\\\\n",
    "    &= \\text{off}(\\mathbf{B}) + \\|\\text{diag}(A)\\|_2^2  + 2a_{pq}^2\n",
    "    &= \\text{off}(\\mathbf{A}) + \\|\\text{diag}(A)\\|_2^2 \n",
    "    &= \\|\\mathbf{A}\\|_{\\text{F}}^2\n",
    "\\end{split}\n",
    "$$\n",
    "or\n",
    "$$\n",
    "\t\\text{off}(\\mathbf{B}) = \\text{off}(\\mathbf{A}) - 2a_{pq}^2 < \\text{off}(\\mathbf{A})\n",
    "$$\n",
    "whenever $a_{pq} \\ne 0$.\n",
    "\n",
    "* One Jacobi rotation costs $O(n)$ flops.\n",
    "\n",
    "* **Classical Jacobi**: search for the largest $|a_{ij}|$ at each iteration. $O(n^2)$ efforts!\n",
    "\n",
    "* $\\text{off}(\\mathbf{A}) \\le n(n-1) a_{ij}^2$ and $\\text{off}(\\mathbf{B}) = \\text{off}(\\mathbf{A}) - 2 a_{ij}^2$ together implies \n",
    "$$\n",
    "\\text{off}(\\mathbf{B}) \\le \\left( 1 - \\frac{2}{n(n-1)} \\right) \\text{off}(\\mathbf{A}).\n",
    "$$\n",
    "Thus Jacobi's method converges at least linearly.\n",
    "\n",
    "* In practice, cyclic-by-row implementation, to avoid the costly $O(n^2)$ search in the classical Jacobi.\n",
    "```Julia\n",
    "[J(p, q, theta) for p=1:n-1, q=p+1:n]\n",
    "```\n",
    "\n",
    "* Jacobi method attracts a lot recent attention because of its rich inherent parallelism (and does not need tridiagonal reduction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "* [The QR algorithm](https://doi.org/10.1109/5992.814656) by Beresford N. Parlett.\n",
    "\n",
    "* Section 8.6 of [Matrix Computations](https://ucla.worldcat.org/title/matrix-computations/oclc/824733531&referer=brief_results) by Gene Golub and Charles Van Loan (2013)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgment\n",
    "\n",
    "Many parts of this lecture note is based on [Dr. Hua Zhou](http://hua-zhou.github.io)'s 2019 Spring Statistical Computing course notes available at <http://hua-zhou.github.io/teaching/biostatm280-2019spring/index.html>."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.4.0",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "341px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "572px",
    "left": "0px",
    "right": "auto",
    "top": "106px",
    "width": "232px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
