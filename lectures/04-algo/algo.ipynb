{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"../..\")\n",
    "Pkg.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms\n",
    "\n",
    "* Algorithm is loosely defined as a set of instructions for doing something, which terminates in finite time. An algorithm requires input and output.\n",
    "\n",
    "* [Donald Knuth](https://en.wikipedia.org/wiki/The_Art_of_Computer_Programming): (1) finiteness, (2) definiteness, (3) input, (4) output, (5) effectiveness.\n",
    "\n",
    "\n",
    "## Measure of efficiency\n",
    "\n",
    "* A basic unit for measuring algorithmic efficiency is **flop**.  \n",
    "> A flop (**floating point operation**) consists of a floating point addition, subtraction, multiplication, division, or comparison, and the usually accompanying fetch and store.  \n",
    "\n",
    "Some books count multiplication followed by an addition (fused multiply-add, FMA) as one flop. This results a factor of up to 2 difference in flop counts.\n",
    "\n",
    "* How to measure efficiency of an algorithm? Big O notation. If $n$ is the size of a problem, an algorithm has order $O(f(n))$, where the leading term in the number of flops is $c \\cdot f(n)$. For example,\n",
    "    - matrix-vector multiplication `A * b`, where `A` is $m \\times n$ and `b` is $n \\times 1$, takes $2mn$ or $O(mn)$ flops  \n",
    "    - matrix-matrix multiplication `A * B`, where `A` is $m \\times n$ and `B` is $n \\times p$, takes $2mnp$ or $O(mnp)$ flops\n",
    "\n",
    "* A hierarchy of computational complexity:  \n",
    "    Let $n$ be the problem size.\n",
    "    - Exponential order: $O(b^n)$ (\"horrible\")    \n",
    "    - Polynomial order: $O(n^q)$ (doable)  \n",
    "    - $O(n \\log n )$ (fast)  \n",
    "    - Linear order $O(n)$ (fast)  \n",
    "    - Logarithmic order $O(\\log n)$ (super fast)  \n",
    "    \n",
    "* Classification of data sets by [Huber](http://link.springer.com/chapter/10.1007%2F978-3-642-52463-9_1) (1994).\n",
    "\n",
    "| Data Size | Bytes     | Storage Mode          |\n",
    "|-----------|-----------|-----------------------|\n",
    "| Tiny      | $10^2$    | Piece of paper        |\n",
    "| Small     | $10^4$    | A few pieces of paper |\n",
    "| Medium    | $10^6$ (megabytes)    | A floppy disk         |\n",
    "| Large     | $10^8$    | Hard disk             |\n",
    "| Huge      | $10^9$ (gigabytes)   | Hard disk(s)          |\n",
    "| Massive   | $10^{12}$ (terabytes) | RAID storage          |\n",
    "\n",
    "* Difference of $O(n^2)$ and $O(n\\log n)$ on massive data. Suppose we have a teraflops supercomputer capable of doing $10^{12}$ flops per second. For a problem of size $n=10^{12}$, $O(n \\log n)$ algorithm takes about \n",
    "$$10^{12} \\log (10^{12}) / 10^{12} \\approx 27 \\text{ seconds}.$$ \n",
    "$O(n^2)$ algorithm takes about $10^{12}$ seconds, which is approximately 31710 years!\n",
    "\n",
    "* QuickSort and Fast Fourier Transform (invented by John Tukey) are celebrated algorithms that turn $O(n^2)$ operations into $O(n \\log n)$. Another example is the Strassen's method for matrix multiplication, which turns $O(n^3)$ matrix multiplication into $O(n^{\\log_2 7})$.    \n",
    "\n",
    "* One goal of this course is to get familiar with the flop counts for some common numerical tasks in statistics.   \n",
    "> **The form of a mathematical expression and the way the expression should be evaluated in actual practice may be quite different.**\n",
    "\\\n",
    "    -- James Gentle, *Matrix Algebra*, Springer, New York (2007).\n",
    "\n",
    "\n",
    "* For example, compare flops of the two mathematically equivalent expressions: `A * B * x` and `A * (B * x)` where `A` and `B` are matrices and `x` is a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools, Random\n",
    "\n",
    "Random.seed!(123) # seed\n",
    "n = 1000\n",
    "A = randn(n, n)\n",
    "B = randn(n, n)\n",
    "x = randn(n)\n",
    "\n",
    "# complexity is n^3 + n^2 = O(n^3)\n",
    "@benchmark $A * $B * $x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complexity is n^2 + n^2 = O(n^2)\n",
    "@benchmark $A * ($B * $x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are there the difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of computer systems\n",
    "\n",
    "* **FLOPS**. \n",
    "\n",
    "* For example, my laptop has the Intel i5-6267U (Skylake) CPU with 2 cores runing at 2.90 GHz (cycles per second)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intel Skylake CPUs can do 16 double-precision flops per cylce and 32 single-precision flops per cycle. Then the **theoretical throughput** of my laptop is\n",
    "$$ 2 \\times 2.9 \\times 10^9 \\times 16 = 92.8 \\text{ GFLOPS DP} $$\n",
    "in double precision and\n",
    "$$ 2 \\times 2.9 \\times 10^9 \\times 32 = 185.6 \\text{ GFLOPS SP} $$\n",
    "in single precision. \n",
    "\n",
    "* In Julia, computes the peak flop rate of the computer by using double precision `gemm!`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "LinearAlgebra.peakflops(2^14) # matrix size 2^14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is about 78.4 GFLOTS DP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stability of numerical algorithms\n",
    "\n",
    "* Abstractly, a *problem* can be viewed as function $f: X \\to Y$ where $X$ is a normed vector space of data and $Y$ is a normed vector space of solutions.\n",
    "    - The problem of solving $Ax=b$ for fixed $b$ is $f: A \\mapsto A^{-1}b$ with $X=\\{M\\in\\mathbb{R}^{n\\times n}: M \\text{ is invertible} \\}$ and $Y = \\mathbb{R}^n$.\n",
    "    \n",
    "* An *algorithm* can be viewed as another map $\\tilde{f}: X \\to Y$.\n",
    "    - Example 1: solve $Ax=b$ by GEPP followed by forward and backward substitutions on a digital computer.\n",
    "    - Example 2: solve $Ax=b$ by Gauss-Seidel (an iterative method to come) on a digital computer.\n",
    "    - In both cases, the solutions (in $Y$) are not the same as $A^{-1}b$!\n",
    "    - Algorithms will be affected by at least rounding errors.\n",
    "    \n",
    "* Algorithm $\\tilde{f}$ is said *stable* if\n",
    "$$\n",
    "    \\forall x \\in X, \n",
    "    \\exists \\tilde{x} \\in X \\text{ such that }\n",
    "    \\frac{\\|\\tilde{x} - x\\|}{\\|x\\|} = O(\\epsilon_{\\max})\n",
    "    \\implies\n",
    "    \\frac{\\|\\tilde{f}(\\tilde{x}) - f(\\tilde{x})\\|}{\\|f(\\tilde{x})\\|} = O(\\epsilon_{\\max})\n",
    "    .\n",
    "$$\n",
    "In words, a stable algorithm gives \"nearly the right\" answer to a \"slightly wrong\" question.\n",
    "\n",
    "* Backward stability: algorithm $\\tilde{f}$ is said *backward stable* if\n",
    "$$\n",
    "    \\forall x \\in X, \n",
    "    \\exists \\tilde{x} \\in X \\text{ such that }\n",
    "    \\frac{\\|\\tilde{x} - x\\|}{\\|x\\|} = O(\\epsilon_{\\max})\n",
    "    \\implies\n",
    "    \\tilde{f}(x) = f(\\tilde{x})\n",
    "    .\n",
    "$$\n",
    "In words, a backward stable algorithm gives \"exactly the right\" answer to a \"slightly wrong\" question.\n",
    "\n",
    "* Examples\n",
    "    - Computing the inner product $x^Ty$ of vectors $x$ snd $y$ using by $[\\sum_{i=1}^n [[x_i][y_i]]]$ (in IEEE754) is backward stable.\n",
    "    - Computing the inner product $A=xy^T$ of vectors $x$ snd $y$ using by $A_{ij}=[[x_i][y_i]]$ (in IEEE754) is  stable but *not* backward stable.\n",
    "    \n",
    "* It can be shown that if a backward stable algorithm $\\tilde{f}$ is applied to solve a problem $f$, the accuarcy of $\\tilde{f}$ is bounded by the condition number of problem $f$.\n",
    "\n",
    "* (Backward) Stability a property of an algorithm, whereas conditioning is a property of a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgment\n",
    "\n",
    "Many parts of this lecture note are based on [Dr. Hua Zhou](http://hua-zhou.github.io)'s 2019 Spring Statistical Computing course notes available at <http://hua-zhou.github.io/teaching/biostatm280-2019spring/index.html>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.0",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "67px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
