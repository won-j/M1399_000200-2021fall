{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semidefinite Programming (SDP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 1.4.0\n",
      "Commit b8e9a9ecc6 (2020-03-21 16:36 UTC)\n",
      "Platform Info:\n",
      "  OS: macOS (x86_64-apple-darwin18.6.0)\n",
      "  CPU: Intel(R) Core(TM) i5-8279U CPU @ 2.40GHz\n",
      "  WORD_SIZE: 64\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-8.0.1 (ORCJIT, skylake)\n"
     ]
    }
   ],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m Activating\u001b[22m\u001b[39m environment at `~/Dropbox/class/M1399.000200/2020/Project.toml`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mStatus\u001b[22m\u001b[39m `~/Dropbox/class/M1399.000200/2020/Project.toml`\n",
      " \u001b[90m [7d9fca2a]\u001b[39m\u001b[37m Arpack v0.4.0\u001b[39m\n",
      " \u001b[90m [6e4b80f9]\u001b[39m\u001b[37m BenchmarkTools v0.5.0\u001b[39m\n",
      " \u001b[90m [1e616198]\u001b[39m\u001b[37m COSMO v0.7.7\u001b[39m\n",
      " \u001b[90m [f65535da]\u001b[39m\u001b[37m Convex v0.13.7\u001b[39m\n",
      " \u001b[90m [31c24e10]\u001b[39m\u001b[37m Distributions v0.23.12\u001b[39m\n",
      " \u001b[90m [f6369f11]\u001b[39m\u001b[37m ForwardDiff v0.10.12\u001b[39m\n",
      " \u001b[90m [c91e804a]\u001b[39m\u001b[37m Gadfly v1.3.1\u001b[39m\n",
      " \u001b[90m [bd48cda9]\u001b[39m\u001b[37m GraphRecipes v0.5.4\u001b[39m\n",
      " \u001b[90m [2e9cd046]\u001b[39m\u001b[37m Gurobi v0.9.2\u001b[39m\n",
      " \u001b[90m [82e4d734]\u001b[39m\u001b[37m ImageIO v0.3.1\u001b[39m\n",
      " \u001b[90m [6218d12a]\u001b[39m\u001b[37m ImageMagick v1.1.6\u001b[39m\n",
      " \u001b[90m [916415d5]\u001b[39m\u001b[37m Images v0.23.1\u001b[39m\n",
      " \u001b[90m [b6b21f68]\u001b[39m\u001b[37m Ipopt v0.6.3\u001b[39m\n",
      " \u001b[90m [42fd0dbc]\u001b[39m\u001b[37m IterativeSolvers v0.8.4\u001b[39m\n",
      " \u001b[90m [4076af6c]\u001b[39m\u001b[37m JuMP v0.21.4\u001b[39m\n",
      " \u001b[90m [b51810bb]\u001b[39m\u001b[37m MatrixDepot v0.9.0-DEV #master (https://github.com/JuliaMatrices/MatrixDepot.jl.git)\u001b[39m\n",
      " \u001b[90m [6405355b]\u001b[39m\u001b[37m Mosek v1.1.3\u001b[39m\n",
      " \u001b[90m [1ec41992]\u001b[39m\u001b[37m MosekTools v0.9.4\u001b[39m\n",
      " \u001b[90m [76087f3c]\u001b[39m\u001b[37m NLopt v0.6.1\u001b[39m\n",
      " \u001b[90m [47be7bcc]\u001b[39m\u001b[37m ORCA v0.5.0\u001b[39m\n",
      " \u001b[90m [a03496cd]\u001b[39m\u001b[37m PlotlyBase v0.4.1\u001b[39m\n",
      " \u001b[90m [f0f68f2c]\u001b[39m\u001b[37m PlotlyJS v0.14.0\u001b[39m\n",
      " \u001b[90m [91a5bcdd]\u001b[39m\u001b[37m Plots v1.6.12\u001b[39m\n",
      " \u001b[90m [438e738f]\u001b[39m\u001b[37m PyCall v1.92.1\u001b[39m\n",
      " \u001b[90m [d330b81b]\u001b[39m\u001b[37m PyPlot v2.9.0\u001b[39m\n",
      " \u001b[90m [dca85d43]\u001b[39m\u001b[37m QuartzImageIO v0.7.3\u001b[39m\n",
      " \u001b[90m [6f49c342]\u001b[39m\u001b[37m RCall v0.13.9\u001b[39m\n",
      " \u001b[90m [c946c3f1]\u001b[39m\u001b[37m SCS v0.7.1\u001b[39m\n",
      " \u001b[90m [276daf66]\u001b[39m\u001b[37m SpecialFunctions v0.10.3\u001b[39m\n",
      " \u001b[90m [2913bbd2]\u001b[39m\u001b[37m StatsBase v0.33.2\u001b[39m\n",
      " \u001b[90m [b8865327]\u001b[39m\u001b[37m UnicodePlots v1.3.0\u001b[39m\n",
      " \u001b[90m [8f399da3]\u001b[39m\u001b[37m Libdl \u001b[39m\n",
      " \u001b[90m [2f01184e]\u001b[39m\u001b[37m SparseArrays \u001b[39m\n",
      " \u001b[90m [10745b16]\u001b[39m\u001b[37m Statistics \u001b[39m\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"../..\")\n",
    "Pkg.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDP\n",
    "\n",
    "* Consider a linear map from $\\mathbb{R}^n$ to $\\mathbb{S}^d$\n",
    "$$\n",
    "    \\mathcal{A}: \\mathbf{x} \\mapsto x_1 \\mathbf{F}_1 + \\cdots + x_n \\mathbf{F}_n.\n",
    "$$\n",
    "  A **linear matrix inequality** (LMI) refers to $\\mathcal{A}\\mathbf{x} + \\mathbf{G} \\preceq \\mathbf{0}$ or $\\mathcal{A}\\mathbf{x} + \\mathbf{G} \\succeq \\mathbf{0}$. Here, $\\mathbf{G}, \\mathbf{F}_1, \\ldots, \\mathbf{F}_n \\in \\mathbb{S}^d$.\n",
    "\n",
    "* A **semidefinite program (SDP)** has the form\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& \\mathbf{c}^T \\mathbf{x} \\\\\n",
    "\t&\\text{subject to}& \\mathcal{A}\\mathbf{x} + \\mathbf{G} \\preceq \\mathbf{0}\\\\\n",
    "\t& & \\mathbf{A} \\mathbf{x} = \\mathbf{b},\n",
    "\\end{eqnarray*}\n",
    "where $\\mathbf{A} \\in \\mathbb{R}^{p \\times n}$, and $\\mathbf{b} \\in \\mathbb{R}^p$. The optimization variable is $\\mathbf{x}$.\n",
    "\n",
    "    When $\\mathbf{G}, \\mathbf{F}_1, \\ldots, \\mathbf{F}_n$ are all diagonal, SDP reduces to LP. (Why?)\n",
    "\n",
    "* The **standard form SDP** has form\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& \\text{tr}(\\mathbf{C} \\mathbf{X}) \\\\\n",
    "\t&\\text{subject to}& \\text{tr}(\\mathbf{A}_i \\mathbf{X}) = b_i, \\quad i = 1, \\ldots, p \\\\\n",
    "\t& & \\mathbf{X} \\succeq \\mathbf{0},\n",
    "\\end{eqnarray*}\n",
    "where $\\mathbf{C}, \\mathbf{A}_1, \\ldots, \\mathbf{A}_p \\in \\mathbb{S}^d$.\n",
    "Note both the objective and the equality constraint are linear in the optimization variable $\\mathbf{X}\\in\\mathbb{S}^d$: $\\text{tr}(\\mathbf{A}\\mathbf{X})=\\langle \\mathbf{A}, \\mathbf{X} \\rangle$. (Why?)\n",
    "\n",
    "* An **inequality form SDP** has form\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& \\mathbf{c}^T \\mathbf{x} \\\\\n",
    "\t&\\text{subject to}& x_1 \\mathbf{A}_1 + \\cdots + x_n \\mathbf{A}_n \\preceq \\mathbf{B},\n",
    "\\end{eqnarray*}\n",
    "with variable $\\mathbf{x} \\in \\mathbb{R}^n$, and parameters $\\mathbf{B}, \\mathbf{A}_1, \\ldots, \\mathbf{A}_n \\in \\mathbb{S}^d$, $\\mathbf{c} \\in \\mathbb{R}^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDr\n",
    "\n",
    "* A convex set $S \\in \\mathbb{R}^n$ is said to be *semidefinite representable* (SDr) if $S$ is the projection of a set in a higher-dimensional space that can be described by a set of LMIs. Specifically,\n",
    "$$\n",
    "    \\mathbf{x}\\in S \\iff \\exists\\mathbf{u}\\in \\mathbb{R}^m: \\mathcal{A}\\begin{pmatrix}\\mathbf{x}\\\\\\mathbf{u}\\end{pmatrix} - \\mathbf{B} \\succeq \\mathbf{0}\n",
    "$$\n",
    "where $\\mathcal{A}: \\mathbb{R}^n\\times \\mathbb{R}^m$ is a linear map.\n",
    "\n",
    "* A convex function $f: \\mathbb{R}^n \\to \\mathbb{R}$ is said to be SDr if and only if $\\text{epi}f$ is SDr.\n",
    "\n",
    "* Any SOCr set is SDr, because\n",
    "$$\n",
    "    \\|\\mathbf{x}\\|_2 \\le t \n",
    "    \\iff\n",
    "    \\mathbf{x}^T\\mathbf{x} \\le t^2, ~ t \\ge 0\n",
    "    \\iff\n",
    "    \\begin{bmatrix} t\\mathbf{I} & \\mathbf{x} \\\\ \\mathbf{x}^T & t \\end{bmatrix} \\succeq \\mathbf{0}\n",
    "$$\n",
    "the last expression is an LMI; this is due to the Schur complement.\n",
    "    - Consequence: any SOCP is SDP.\n",
    "\n",
    "* Exercise. Write LP, QP, QCQP, and SOCP in form of SDP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDP example: nearest correlation matrix\n",
    "\n",
    "* Let $\\mathbb{C}^p$ be the convex set of $p \\times p$ correlation matrices\n",
    "$$\n",
    "\t\\mathbb{C}^p = \\{ \\mathbf{X} \\in \\mathbb{S}_+^p: x_{ii}=1, i=1,\\ldots,p\\}.\n",
    "$$\n",
    "Given $\\mathbf{A} \\in \\mathbb{S}^p$, often we need to find the closest correlation matrix to $\\mathbf{A}$\n",
    "$$\n",
    "    \\begin{array}{ll}\n",
    "\t\\text{minimize}& \\|\\mathbf{A} - \\mathbf{X}\\|_{\\text{F}} \\\\\n",
    "\t\\text{subject to}& \\mathbf{X} \\in \\mathbb{C}^p.\n",
    "    \\end{array}\n",
    "$$\n",
    "This projection problem can be solved via an SDP\n",
    "$$\n",
    "    \\begin{array}{ll}\n",
    "\t\\text{minimize}& t \\\\\n",
    "\t\\text{subject to}& \\|\\mathbf{A} - \\mathbf{X}\\|_{\\text{F}} \\le t \\\\\n",
    "\t & \\text{diag}(\\mathbf{X}) = \\mathbf{1} \\\\\n",
    "\t & \\mathbf{X} \\succeq \\mathbf{0}\n",
    "    \\end{array}\n",
    "$$\n",
    "in variables $\\mathbf{X} \\in \\mathbb{S}^{p}$ and $t \\in \\mathbb{R}$. \n",
    "    - The SOC constraint $\\|\\mathbf{A} - \\mathbf{X}\\|_{\\text{F}} \\le t$ can be written as an LMI\n",
    "$$\n",
    "\t\\begin{bmatrix}\n",
    "\t\tt \\mathbf{I} & \\text{vec} (\\mathbf{A} - \\mathbf{X}) \\\\\n",
    "\t\t\\text{vec} (\\mathbf{A} - \\mathbf{X})^T & t\n",
    "\t\\end{bmatrix} \\succeq \\mathbf{0}\n",
    "$$\n",
    "(why?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       "  1.0        0.502452    0.218669    0.355791  -0.146157\n",
       "  0.502452   1.0         0.0731595   0.456861  -0.071196\n",
       "  0.218669   0.0731595   1.0        -0.523404   0.570529\n",
       "  0.355791   0.456861   -0.523404    1.0       -0.318838\n",
       " -0.146157  -0.071196    0.570529   -0.318838   1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Random, LinearAlgebra, Convex\n",
    "\n",
    "Random.seed!(123) # seed\n",
    "n=5\n",
    "A=randn(n,n)\n",
    "ludecomp=lu(A)\n",
    "P=ludecomp.L*transpose(ludecomp.L)\n",
    "Drt=Diagonal(1 ./sqrt.(diag(P)))\n",
    "C = Drt * P * Drt  # correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       " 0.16360136405405437\n",
       " 0.43755046781021156\n",
       " 0.6769093573763024\n",
       " 1.6020079604009598\n",
       " 2.1199308503584726"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigvals(C)  # positive definite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       "  1.76196     0.0669233   0.252147  -0.20663   -0.259963\n",
       "  0.0669233   1.17286    -0.798939   0.284663  -0.164121\n",
       "  0.252147   -0.798939    0.559738  -0.644653   1.33644\n",
       " -0.20663     0.284663   -0.644653   1.766     -1.89047\n",
       " -0.259963   -0.164121    1.33644   -1.89047    0.279969"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E  = randn(n, n) * 5e-1  \n",
    "A = C + Symmetric(E)   # perturbed correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       " -1.45975254459313\n",
       "  0.0998733891717517\n",
       "  1.315576088882116\n",
       "  1.8115884261931743\n",
       "  3.773240675591988"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigvals(A) # indefinite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 23.877823 seconds (74.86 M allocations: 3.690 GiB, 7.52% gc time)\n"
     ]
    }
   ],
   "source": [
    "# Use Mosek solver\n",
    "using Mosek, MosekTools\n",
    "opt = () -> Mosek.Optimizer(LOG=0)\n",
    "\n",
    "## Use SCS solver\n",
    "#using SCS\n",
    "#opt = () -> SCS.Optimizer(verbose=0)  \n",
    "\n",
    "## Use COSMO solver\n",
    "#using COSMO\n",
    "#opt = () -> COSMO.Optimizer(max_iter=10000, verbose=false) \n",
    "\n",
    "X = Semidefinite(n)\n",
    "problem = minimize(norm(vec(A - X)))  # Frobenius norm\n",
    "problem.constraints += diag(X) == ones(n)\n",
    "# Solve the problem by calling solve\n",
    "@time solve!(problem, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       "  1.0         0.034539   0.159093  -0.0713063  -0.0522196\n",
       "  0.034539    1.0       -0.675227   0.236175   -0.306471\n",
       "  0.159093   -0.675227   1.0       -0.866109    0.878962\n",
       " -0.0713063   0.236175  -0.866109   1.0        -0.990035\n",
       " -0.0522196  -0.306471   0.878962  -0.990035    1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix is positive semidefinite up to numerical precision of the solver.\n",
    "\n",
    "- Compare the accuracy of MOSEK (interior point solver) and SCS/COSMO (first-order solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       " -3.9021663660333345e-9\n",
       "  1.875080310278968e-8\n",
       "  0.9072291325612191\n",
       "  1.0248116633257551\n",
       "  3.067959189264389"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigvals(X.value)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For the history of the nearest matrix problem and algorithms, check out [Nick Higham's blog](https://nhigham.com/2013/02/13/the-nearest-correlation-matrix/)\n",
    "\n",
    "* A variant of the nearest matrix problem has an intimate connection with joint estimation of Gaussian mean vector and covariance matrix:\n",
    "    - Joong-Ho Won, [Proximity Operator of the Matrix Perspective Function and its Applications](https://proceedings.neurips.cc/paper/2020/file/45f31d16b1058d586fc3be7207b58053-Paper.pdf), NeurIPS 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDP example: eigenvalue problems\n",
    "\n",
    "* Suppose \n",
    "$$\n",
    "\t\\mathbf{A}(\\mathbf{x}) = \\mathbf{A}_0 + x_1 \\mathbf{A}_1 + \\cdots x_n \\mathbf{A}_n,\n",
    "$$\n",
    "where $\\mathbf{A}_i \\in \\mathbb{S}^m$, $i = 0, \\ldots, n$. Let $\\lambda_1(\\mathbf{x}) \\ge \\lambda_2(\\mathbf{x}) \\ge \\cdots \\ge \\lambda_m(\\mathbf{x})$ be the ordered eigenvalues of $\\mathbf{A}(\\mathbf{x})$.\n",
    "\n",
    "* Minimizing the maximal eigenvalue is equivalent to the SDP\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& t \\\\\n",
    "\t&\\text{subject to}& \\mathbf{A}(\\mathbf{x}) \\preceq t \\mathbf{I}\n",
    "\\end{eqnarray*}\n",
    "in variables $\\mathbf{x} \\in \\mathbb{R}^n$ and $t \\in \\mathbb{R}$.\n",
    "\n",
    "    - Minimizing the sum of $k$ largest eigenvalues is an SDP too. How about minimizing the sum of all eigenvalues?\n",
    "    - *Maximizing* the minimum eigenvalue is an SDP as well.\n",
    "\n",
    "* Minimizing the *spread* of the eigenvalues $\\lambda_1(\\mathbf{x}) - \\lambda_m(\\mathbf{x})$ is equivalent to the SDP\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& t_1 - t_m \\\\\n",
    "\t&\\text{subject to}& t_m \\mathbf{I} \\preceq \\mathbf{A}(\\mathbf{x}) \\preceq t_1 \\mathbf{I}\n",
    "\\end{eqnarray*}\n",
    "in variables $\\mathbf{x} \\in \\mathbb{R}^n$ and $t_1, t_m \\in \\mathbb{R}$.\n",
    "\n",
    "* Minimizing the spectral radius (or spectral norm) $\\rho(\\mathbf{x}) = \\max_{i=1,\\ldots,m} |\\lambda_i(\\mathbf{x})|$ is equivalent to the SDP\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& t \\\\\n",
    "\t&\\text{subject to}& - t \\mathbf{I} \\preceq \\mathbf{A}(\\mathbf{x}) \\preceq t \\mathbf{I}\n",
    "\\end{eqnarray*}\n",
    "in variables $\\mathbf{x} \\in \\mathbb{R}^n$ and $t \\in \\mathbb{R}$.\n",
    "\n",
    "* To minimize the condition number $\\kappa(\\mathbf{x}) = \\lambda_1(\\mathbf{x}) / \\lambda_m(\\mathbf{x})$, note $\\lambda_1(\\mathbf{x}) / \\lambda_m(\\mathbf{x}) \\le t$ if and only if there exists a $\\mu > 0$ such that $\\mu \\mathbf{I} \\preceq \\mathbf{A}(\\mathbf{x}) \\preceq \\mu t \\mathbf{I}$, or equivalently, $\\mathbf{I} \\preceq \\mu^{-1} \\mathbf{A}(\\mathbf{x}) \\preceq t \\mathbf{I}$. With change of variables $y_i = x_i / \\mu$ and $s = 1/\\mu$, we can solve the SDP\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& t \\\\\n",
    "\t&\\text{subject to}& \\mathbf{I} \\preceq s \\mathbf{A}_0 + y_1 \\mathbf{A}_1 + \\cdots y_n \\mathbf{A}_n \\preceq t \\mathbf{I} \\\\\n",
    "\t& & s \\ge 0,\n",
    "\\end{eqnarray*}\n",
    "in variables $\\mathbf{y} \\in \\mathbb{R}^n$ and $s, t \\ge 0$. In other words, we normalize the spectrum by the smallest eigenvalue and then minimize the largest eigenvalue of the normalized LMI.\n",
    "\n",
    "\n",
    "* Minimizing the $\\ell_1$ norm of the eigenvalues $|\\lambda_1(\\mathbf{x})| + \\cdots + |\\lambda_m(\\mathbf{x})|$ is equivalent to the SDP\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& \\text{tr} (\\mathbf{A}^+) + \\text{tr}(\\mathbf{A}^-) \\\\\n",
    "\t&\\text{subject to}& \\mathbf{A}(\\mathbf{x}) = \\mathbf{A}^+ - \\mathbf{A}^- \\\\\n",
    "\t& & \\mathbf{A}^+ \\succeq \\mathbf{0}, \\mathbf{A}^- \\succeq \\mathbf{0},\n",
    "\\end{eqnarray*}\n",
    "in variables $\\mathbf{x} \\in \\mathbb{R}^n$ and $\\mathbf{A}^+, \\mathbf{A}^- \\in \\mathbb{S}_+^n$.\n",
    "\n",
    "* Roots of determinant. The determinant of a semidefinite matrix $\\text{det} (\\mathbf{A}(\\mathbf{x})) = \\prod_{i=1}^m \\lambda_i (\\mathbf{x})$ is neither convex or concave, but **rational powers** of the determinant can be modeled using linear matrix inequalities. For a rational power $0 \\le q \\le 1/m$, the function $\\text{det} (\\mathbf{A}(\\mathbf{x}))^q$ is concave and we have\n",
    "\\begin{eqnarray*}\n",
    "\t& & t \\le \\text{det} (\\mathbf{A}(\\mathbf{x}))^q\\\\\n",
    "\t&\\Leftrightarrow& \\begin{bmatrix}\n",
    "\t\\mathbf{A}(\\mathbf{x}) & \\mathbf{L} \\\\\n",
    "\t\\mathbf{L}^T & \\text{diag}(\\mathbf{L})\n",
    "\t\\end{bmatrix} \\succeq \\mathbf{0}, \\quad (\\ell_{11} \\ell_{22} \\cdots \\ell_{mm})^q \\ge t,\n",
    "\\end{eqnarray*}\n",
    "where $\\mathbf{L} \\in \\mathbb{R}^{m \\times m}$ is a lower-triangular matrix. The first inequality is an LMI, and the second is SOCr.\n",
    "\n",
    "  Similarly for any rational $q>0$, we have\n",
    "\\begin{eqnarray*}\n",
    "\t& & t \\ge \\text{det} (\\mathbf{A}(\\mathbf{x}))^{-q} \\\\\n",
    "\t&\\Leftrightarrow& \\begin{bmatrix}\n",
    "\t\\mathbf{A}(\\mathbf{x}) & \\mathbf{L} \\\\\n",
    "\t\\mathbf{L}^T & \\text{diag}(\\mathbf{L})\n",
    "\t\\end{bmatrix} \\succeq \\mathbf{0}, \\quad (\\ell_{11} \\ell_{22} \\cdots \\ell_{mm})^{-q} \\le t\n",
    "\\end{eqnarray*}\n",
    "for a lower triangular $\\mathbf{L}$.\n",
    "\n",
    "* References: See Lecture 4 (p146-p151) in the book [Ben-Tal and Nemirovski (2001)](https://doi.org/10.1137/1.9780898718829) for the proof of above facts.\n",
    "\n",
    "* `lambda_max`, `lambda_min`, `lambda_sum_largest`, `lambda_sum_smallest`, `det_rootn`, and `trace_inv` are implemented in cvx for Matlab.\n",
    "\n",
    "* `lambda_max`, `lambda_min` are implemented in Convex.jl package for Julia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical applications\n",
    "\n",
    "* Covariance matrix estimation\n",
    "\n",
    "  Suppose $\\mathbf{x}_1, \\dotsc, \\mathbf{x}_n \\in \\mathbb{R}^p \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma})$ and we want to estimate the covariance matrix $\\Sigma$ by maximium likelihood. If $n \\ge p$, then the sample covariance matrix $\\mathbf{S}=\\frac{1}{n}\\sum_{i=1}^n\\mathbf{x}_i\\mathbf{x}_i^T$ is the MLE. However, if $n < p$ (high-dimensional setting), the MLE is not defined (why?)\n",
    "  \n",
    "  A possiblity is to *regularize* the MLE, e.g., by restricting the condition number of $\\boldsymbol\\Sigma$ to be less than $\\kappa$. This approach leads to an SDP\n",
    "$$\n",
    "    \\begin{array}{ll}\n",
    "    \\text{maximize} & \\log\\text{det}\\boldsymbol{\\Sigma}^{-1} - \\text{tr}(\\boldsymbol{\\Sigma}^{-1}\\mathbf{S}) \\\\\n",
    "    \\text{subject to} & \\mu\\mathbf{I} \\preceq \\boldsymbol{\\Sigma}^{-1} \\preceq \\kappa\\mu\\mathbf{I}\n",
    "    \\end{array}\n",
    "$$\n",
    "where the optimization variables are $\\boldsymbol{\\Sigma}^{-1}\\in\\mathbb{S}^p$ and $\\mu\\in\\mathbb{R}_+$. See\n",
    "    - Won, J.H., Lim, J., Kim, S.J. and Rajaratnam, B., 2013. Condition‐number‐regularized covariance estimation. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 75(3), pp.427-450. <https://doi.org/10.1111/j.1467-9868.2012.01049.x>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Optimal experiment design    \n",
    "\n",
    "  Consider a linear model\n",
    "$$\n",
    "y_i = \\mathbf{x}_i^T\\boldsymbol{\\beta} + \\varepsilon_i, \\quad i=1,\\dotsc,n,\n",
    "$$\n",
    "where $\\varepsilon_i$ are independent Gaussian noises with common variance $\\sigma^2$. \n",
    "It is well known that the least squares estimate $\\hat{\\boldsymbol{\\beta}}$ is unbiased and has covariance $\\sigma^2(\\sum_{i=1}^n\\mathbf{x}_i\\mathbf{x}_i ^T)^{−1}$. \n",
    "\n",
    "  In experimental design, given total number of allowable experiments, we want to choose among a list of $m$ candidate design points $\\{\\mathbf{x}_1,\\dotsc,\\mathbf{x}_m\\}$ such that the covariance matrix is *minimized* in some sense. This *exact* design problem is a combinatorial optimization problem, which is hard to solve. Instead, weight $w_i$ is put to each design point $\\mathbf{x}_i$, and we want to find a probability vector $\\mathbf{w}\\in\\Delta_{m-1}$, and the matrix $V = \\sum_{i=1}^m (w_i\\mathbf{x}_i\\mathbf{x}_i^T)^{−1}$ is \"small\". Some popular criteria are:\n",
    "    - D-optimal design: minimize $\\text{det}V$.\n",
    "    - E-optimal design: minimize $\\|V\\|_2 = \\lambda_{\\max}(V)$.\n",
    "    - A-optimal design: minimize $\\frac{1}{m}\\sum_{i=1}^p\\lambda_i(V) = \\text{trace}{V}$.\n",
    "    \n",
    "  All of these problems are can be cast as SDPs; see\n",
    "    - Vandenberghe, L., Boyd, S. and Wu, S.P., 1998. Determinant maximization with linear matrix inequality constraints. SIAM journal on matrix analysis and applications, 19(2), pp.499-533. <https://doi.org/10.1137/S0895479896303430>\n",
    "    - Papp, D., 2012. Optimal designs for rational function regression. Journal of the American Statistical Association, 107(497), pp.400-411. <https://doi.org/10.1080/01621459.2012.656035>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDP example: singular value problems\n",
    "\n",
    "* Let $\\mathbf{A}(\\mathbf{x}) = \\mathbf{A}_0 + x_1 \\mathbf{A}_1 + \\cdots x_n \\mathbf{A}_n$, where $\\mathbf{A}_i \\in \\mathbb{R}^{p \\times q}$ and $\\sigma_1(\\mathbf{x}) \\ge \\cdots \\sigma_{\\min\\{p,q\\}}(\\mathbf{x}) \\ge 0$ be the ordered singular values.\n",
    "\n",
    "* **Spectral norm** (or **operator norm** or **matrix-2 norm**) minimization.  Consider minimizing  the spectral norm $\\|\\mathbf{A}(\\mathbf{x})\\|_2 = \\sigma_1(\\mathbf{x})$. Note $\\|\\mathbf{A}\\|_2 \\le t$ if and only if $\\mathbf{A}^T \\mathbf{A} \\preceq t^2 \\mathbf{I}$ (and $t \\ge 0$) if and only if $\\begin{bmatrix} t\\mathbf{I} & \\mathbf{A} \\\\ \\mathbf{A}^T & t \\mathbf{I} \\end{bmatrix} \\succeq \\mathbf{0}$. This results in the SDP\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& t \\\\\n",
    "\t&\\text{subject to}& \\begin{bmatrix} t\\mathbf{I} & \\mathbf{A}(\\mathbf{x}) \\\\ \\mathbf{A}(\\mathbf{x})^T & t \\mathbf{I} \\end{bmatrix} \\succeq \\mathbf{0}\n",
    "\\end{eqnarray*}\n",
    "in variables $\\mathbf{x} \\in \\mathbb{R}^n$ and $t \\in \\mathbb{R}$.\n",
    "  \n",
    "\n",
    "* **Nuclear norm** minimization. Minimization of the *nuclear norm* (or *trace norm*) $\\|\\mathbf{A}(\\mathbf{x})\\|_* = \\sum_i \\sigma_i(\\mathbf{x})$ can be formulated as an SDP. \n",
    "\n",
    "  Singular values of $\\mathbf{A}$ coincides with the eigenvalues of the symmetric matrix\n",
    "$$\n",
    "    \\bar{\\mathbb{A}} =\n",
    "\t\\begin{bmatrix}\n",
    "\t\\mathbf{0} & \\mathbf{A} \\\\\n",
    "\t\\mathbf{A}^T & \\mathbf{0}\n",
    "\t\\end{bmatrix},\n",
    "$$\n",
    "which has eigenvalues $(\\sigma_1, \\ldots, \\sigma_p, - \\sigma_p, \\ldots, - \\sigma_1)$. Therefore minimizing the nuclear norm of $\\mathbf{A}$ is same as minimizing the $\\ell_1$ norm of eigenvalues of $\\bar{\\mathbf{A}}$, which we know is an SDP.\n",
    "\n",
    "    - Minimizing the sum of $k$ largest singular values is an SDP as well.\n",
    "    \n",
    "* `sigma_max` and `norm_nuc` are implemented in cvx for Matlab.\n",
    "\n",
    "* `operator_norm` and `nuclear_norm` are implemented in Convex.jl package for Julia.\n",
    "\n",
    "### Statistical applications\n",
    "\n",
    "* Matrix completion (we've seen this before).\n",
    "\n",
    "* (Approximately) rank-regularized covariance matrix estimation.\n",
    "$$\n",
    "    \\begin{array}{ll}\n",
    "    \\text{maximize} & \\log\\text{det}\\boldsymbol{\\Omega} - \\text{tr}(\\boldsymbol{\\Omega}\\mathbf{S})  \\\\\n",
    "    \\text{subject to} & \\|\\boldsymbol{\\Omega}\\|_* \\le k\n",
    "    \\end{array}\n",
    "$$\n",
    "where $\\boldsymbol{\\Omega}=\\boldsymbol{\\Sigma}^{-1}$.\n",
    "    - This approach yields $\\mathbf{S} + \\kappa\\mathbf{I}$ as a solution. See\n",
    "    \n",
    "      Warton, D.I., 2008. Penalized normal likelihood and ridge regularization of correlation and covariance matrices. Journal of the American Statistical Association, 103(481), pp.340-349. <https://doi.org/10.1198/016214508000000021>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDP example: cone of nonnegative polynomials\n",
    "\n",
    "* Consider nonnegative polynomial of degree $2n$ \n",
    "\\begin{eqnarray*}\n",
    "\tf(t) = \\mathbf{x}^T \\mathbf{v}(t) = x_0 + x_1 t + \\cdots x_{2n} t^{2n} \\ge 0, \\text{ for all } t.\n",
    "\\end{eqnarray*}\n",
    "The cone\n",
    "\\begin{eqnarray*}\n",
    "\t\\mathbf{K}^n = \\{\\mathbf{x} \\in \\mathbb{R}^{2n+1}: f(t) = \\mathbf{x}^T \\mathbf{v}(t) \\ge 0, \\text{ for all } t \\in \\mathbb{R}\\}\n",
    "\\end{eqnarray*}\n",
    "can be characterized by LMI\n",
    "\\begin{eqnarray*}\n",
    "\tf(t) \\ge 0 \\text{ for all } t \\Leftrightarrow x_i = \\langle \\mathbf{X}, \\mathbf{H}_i \\rangle, i=0,\\ldots,2n, \\mathbf{X} \\in \\mathbb{S}_+^{n+1},\n",
    "\\end{eqnarray*}\n",
    "where $\\mathbf{H}_i \\in \\mathbb{R}^{(n+1) \\times (n+1)}$ are Hankel matrices with entries $(\\mathbf{H}_i)_{kl} = 1$ if $k+l=i$ or 0 otherwise. Here $k, l \\in \\{0, 1, \\ldots, n\\}$. \n",
    "\n",
    "* Similarly the cone of nonnegative polynomials on a finite interval\n",
    "\\begin{eqnarray*}\n",
    "\t\\mathbf{K}_{a,b}^n = \\{\\mathbf{x} \\in \\mathbb{R}^{n+1}: f(t) = \\mathbf{x}^T \\mathbf{v}(t) \\ge 0, \\text{ for all } t \\in [a,b]\\}\n",
    "\\end{eqnarray*}\n",
    "can be characterized by LMI as well. MosekModelling.pdf p48.\n",
    "    * (Even degree) Let $n = 2m$. Then\n",
    "    \\begin{eqnarray*}\n",
    "        \\mathbf{K}_{a,b}^n &=& \\{\\mathbf{x} \\in \\mathbb{R}^{n+1}: x_i = \\langle \\mathbf{X}_1, \\mathbf{H}_i^m \\rangle + \\langle \\mathbf{X}_2, (a+b) \\mathbf{H}_{i-1}^{m-1} - ab \\mathbf{H}_i^{m-1} - \\mathbf{H}_{i-2}^{m-1} \\rangle, \\\\\n",
    "        & & \\quad i = 0,\\ldots,n, \\mathbf{X}_1 \\in \\mathbf{S}_+^m, \\mathbf{X}_2 \\in \\mathbb{S}_+^{m-1}\\}.\n",
    "    \\end{eqnarray*}\n",
    "    * (Odd degree) Let $n = 2m + 1$. Then\n",
    "    \\begin{eqnarray*}\n",
    "        \\mathbf{K}_{a,b}^n &=& \\{\\mathbf{x} \\in \\mathbb{R}^{n+1}: x_i = \\langle \\mathbf{X}_1, \\mathbf{H}_{i-1}^m - a \\mathbf{H}_i^m \\rangle + \\langle \\mathbf{X}_2, b \\mathbf{H}_{i}^{m} - \\mathbf{H}_{i-1}^{m} \\rangle, \\\\\n",
    "        & & \\quad i = 0,\\ldots,n, \\mathbf{X}_1, \\mathbf{X}_2 \\in \\mathbb{S}_+^m\\}.\n",
    "    \\end{eqnarray*}\n",
    "\n",
    "* References: [Nesterov (2000)](https://doi.org/10.1007/978-1-4757-3216-0_17) and Lecture 4 (p157-p159) of [Ben-Tal and Nemirovski (2001)](https://doi.org/10.1137/1.9780898718829).\n",
    "\n",
    "* Example. Polynomial curve fitting. We want to fit a univariate polynomial of degree $n$\n",
    "\\begin{eqnarray*}\n",
    "\tf(t) = x_0 + x_1 t + x_2 t^2 + \\cdots x_n t^n\n",
    "\\end{eqnarray*}\n",
    "to a set of measurements $(t_i, y_i)$, $i=1,\\ldots,m$, such that $f(t_i) \\approx y_i$. Define the Vandermonde matrix\n",
    "\\begin{eqnarray*}\n",
    "\t\\mathbf{A} = \\begin{pmatrix}\n",
    "\t1 & t_1 & t_1^2 & \\cdots & t_1^n \\\\\n",
    "\t1 & t_2 & t_2^2 & \\cdots & t_2^n \\\\\n",
    "\t\\vdots & \\vdots & \\vdots & & \\vdots \\\\\n",
    "\t1 & t_m & t_m^2 & \\cdots & t_m^n\n",
    "\t\\end{pmatrix},\n",
    "\\end{eqnarray*}\n",
    "then we wish $\\mathbf{A} \\mathbf{x} \\approx \\mathbf{y}$. Using least squares criterion, we obtain the optimal solution $\\mathbf{x}_{\\text{LS}} = (\\mathbf{A}^T \\mathbf{A})^{-1} \\mathbf{A}^T \\mathbf{y}$. With various constraints, it is possible to find optimal $\\mathbf{x}$ by SDP.\n",
    "    * Nonnegativity. Then we require $\\mathbf{x} \\in \\mathbf{K}_{a,b}^n$.\n",
    "    * Monotonicity. We can ensure monotonicity of $f(t)$ by requiring that $f'(t) \\ge 0$ or $f'(t) \\le 0$. That is $(x_1,2x_2, \\ldots, nx_n) \\in \\mathbf{K}_{a,b}^{n-1}$ or $-(x_1,2x_2, \\ldots, nx_n) \\in \\mathbf{K}_{a,b}^{n-1}$.\n",
    "    * Convexity or concavity. Convexity or concavity of $f(t)$ corresponds to $f''(t) \\ge 0$ or $f''(t) \\le 0$. That is $(2x_2, 2x_3, \\ldots, (n-1)nx_n) \\in \\mathbf{K}_{a,b}^{n-2}$ or $-(2x_2, 2x_3, \\ldots, (n-1)nx_n) \\in \\mathbf{K}_{a,b}^{n-2}$.\n",
    "    \n",
    "* `nonneg_poly_coeffs()` and `convex_poly_coeffs()` are implemented in cvx. Not in Convex.jl yet.\n",
    "\n",
    "* Statistical Applications:\n",
    "    - Kim, S.-J., Lim,  J. , Won, J. H. Nonparametric Sharpe Ratio Function Estimation in Heteroscedastic Regression Models via Convex Optimization. PMLR 84:1495-1504, 2018. <http://proceedings.mlr.press/v84/kim18b.html>\n",
    "    - Papp, D. and F. Alizadeh (2014). Shape-constrained estimation using nonnegative splines. Journal of Computational and Graphical Statistics 23(1), 211– 231. <https://doi.org/10.1080/10618600.2012.707343>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDP relaxation of binary LP\n",
    "\n",
    "* Consider a binary linear programming problem\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& \\mathbf{c}^T \\mathbf{x} \\\\\n",
    "\t&\\text{subject to}& \\mathbf{A} \\mathbf{x} = \\mathbf{b}, \\quad \\mathbf{x} \\in \\{0,1\\}^n.\n",
    "\\end{eqnarray*}\n",
    "Note\n",
    "\\begin{eqnarray*}\n",
    "\t& & x_i \\in \\{0,1\\} \n",
    "\t\\Leftrightarrow x_i^2 = x_i \n",
    "\t\\Leftrightarrow \\mathbf{X} = \\mathbf{x} \\mathbf{x}^T, \\text{diag}(\\mathbf{X}) = \\mathbf{x}.\n",
    "\\end{eqnarray*}\n",
    "By relaxing the rank 1 constraint on $\\mathbf{X}$, we obtain an SDP relaxation\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& \\mathbf{c}^T \\mathbf{x} \\\\\n",
    "\t&\\text{subject to}& \\mathbf{A} \\mathbf{x} = \\mathbf{b}, \\text{diag}(\\mathbf{X}) = \\mathbf{x}, \\mathbf{X} \\succeq \\mathbf{x} \\mathbf{x}^T.\n",
    "\\end{eqnarray*}\n",
    "Note $\\mathbf{X} \\succeq \\mathbf{x} \\mathbf{x}^T$ is equivalent to the LMI \n",
    "\\begin{eqnarray*}\n",
    "\t\\begin{bmatrix}\n",
    "\t1 & \\mathbf{x}^T \\\\ \\mathbf{x} &\\mathbf{X}\n",
    "\t\\end{bmatrix} \\succeq \\mathbf{0}.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "This SDP be efficiently solved and provides a **lower bound** to the original problem (why?). If the optimal $\\mathbf{X}$ has rank 1, then it is a solution to the original binary LP.\n",
    "\n",
    "We can tighten the relaxation by adding other constraints that cut away part of the feasible set, without excluding rank 1 solutions. For instance, $0 \\le x_i \\le 1$ and $0 \\le X_{ij} \\le 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgment\n",
    "\n",
    "Many parts of this lecture note is based on [Dr. Hua Zhou](http://hua-zhou.github.io)'s 2019 Spring Statistical Computing course notes available at <http://hua-zhou.github.io/teaching/biostatm280-2019spring/index.html>."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.4.0",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "180px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "390.3333435058594px",
    "left": "0px",
    "right": "818px",
    "top": "140.6666717529297px",
    "width": "180px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
