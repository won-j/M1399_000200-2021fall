{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 1.4.0\n",
      "Commit b8e9a9ecc6 (2020-03-21 16:36 UTC)\n",
      "Platform Info:\n",
      "  OS: macOS (x86_64-apple-darwin18.6.0)\n",
      "  CPU: Intel(R) Core(TM) i5-8279U CPU @ 2.40GHz\n",
      "  WORD_SIZE: 64\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-8.0.1 (ORCJIT, skylake)\n"
     ]
    }
   ],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m Activating\u001b[22m\u001b[39m environment at `~/Dropbox/class/M1399.000200/2020/Project.toml`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mStatus\u001b[22m\u001b[39m `~/Dropbox/class/M1399.000200/2020/Project.toml`\n",
      " \u001b[90m [7d9fca2a]\u001b[39m\u001b[37m Arpack v0.4.0\u001b[39m\n",
      " \u001b[90m [6e4b80f9]\u001b[39m\u001b[37m BenchmarkTools v0.5.0\u001b[39m\n",
      " \u001b[90m [1e616198]\u001b[39m\u001b[37m COSMO v0.7.7\u001b[39m\n",
      " \u001b[90m [f65535da]\u001b[39m\u001b[37m Convex v0.13.7\u001b[39m\n",
      " \u001b[90m [a93c6f00]\u001b[39m\u001b[37m DataFrames v0.21.8\u001b[39m\n",
      " \u001b[90m [31c24e10]\u001b[39m\u001b[37m Distributions v0.23.12\u001b[39m\n",
      " \u001b[90m [e2685f51]\u001b[39m\u001b[37m ECOS v0.12.1\u001b[39m\n",
      " \u001b[90m [f6369f11]\u001b[39m\u001b[37m ForwardDiff v0.10.12\u001b[39m\n",
      " \u001b[90m [c91e804a]\u001b[39m\u001b[37m Gadfly v1.3.1\u001b[39m\n",
      " \u001b[90m [bd48cda9]\u001b[39m\u001b[37m GraphRecipes v0.5.4\u001b[39m\n",
      " \u001b[90m [2e9cd046]\u001b[39m\u001b[37m Gurobi v0.9.2\u001b[39m\n",
      " \u001b[90m [82e4d734]\u001b[39m\u001b[37m ImageIO v0.3.1\u001b[39m\n",
      " \u001b[90m [6218d12a]\u001b[39m\u001b[37m ImageMagick v1.1.6\u001b[39m\n",
      " \u001b[90m [916415d5]\u001b[39m\u001b[37m Images v0.23.1\u001b[39m\n",
      " \u001b[90m [b6b21f68]\u001b[39m\u001b[37m Ipopt v0.6.3\u001b[39m\n",
      " \u001b[90m [42fd0dbc]\u001b[39m\u001b[37m IterativeSolvers v0.8.4\u001b[39m\n",
      " \u001b[90m [4076af6c]\u001b[39m\u001b[37m JuMP v0.21.4\u001b[39m\n",
      " \u001b[90m [b51810bb]\u001b[39m\u001b[37m MatrixDepot v0.9.0-DEV #master (https://github.com/JuliaMatrices/MatrixDepot.jl.git)\u001b[39m\n",
      " \u001b[90m [6405355b]\u001b[39m\u001b[37m Mosek v1.1.3\u001b[39m\n",
      " \u001b[90m [1ec41992]\u001b[39m\u001b[37m MosekTools v0.9.4\u001b[39m\n",
      " \u001b[90m [76087f3c]\u001b[39m\u001b[37m NLopt v0.6.1\u001b[39m\n",
      " \u001b[90m [47be7bcc]\u001b[39m\u001b[37m ORCA v0.5.0\u001b[39m\n",
      " \u001b[90m [a03496cd]\u001b[39m\u001b[37m PlotlyBase v0.4.1\u001b[39m\n",
      " \u001b[90m [f0f68f2c]\u001b[39m\u001b[37m PlotlyJS v0.14.0\u001b[39m\n",
      " \u001b[90m [91a5bcdd]\u001b[39m\u001b[37m Plots v1.6.12\u001b[39m\n",
      " \u001b[90m [438e738f]\u001b[39m\u001b[37m PyCall v1.92.1\u001b[39m\n",
      " \u001b[90m [d330b81b]\u001b[39m\u001b[37m PyPlot v2.9.0\u001b[39m\n",
      " \u001b[90m [dca85d43]\u001b[39m\u001b[37m QuartzImageIO v0.7.3\u001b[39m\n",
      " \u001b[90m [6f49c342]\u001b[39m\u001b[37m RCall v0.13.9\u001b[39m\n",
      " \u001b[90m [ce6b1742]\u001b[39m\u001b[37m RDatasets v0.6.10\u001b[39m\n",
      " \u001b[90m [c946c3f1]\u001b[39m\u001b[37m SCS v0.7.1\u001b[39m\n",
      " \u001b[90m [276daf66]\u001b[39m\u001b[37m SpecialFunctions v0.10.3\u001b[39m\n",
      " \u001b[90m [2913bbd2]\u001b[39m\u001b[37m StatsBase v0.33.2\u001b[39m\n",
      " \u001b[90m [b8865327]\u001b[39m\u001b[37m UnicodePlots v1.3.0\u001b[39m\n",
      " \u001b[90m [8f399da3]\u001b[39m\u001b[37m Libdl \u001b[39m\n",
      " \u001b[90m [2f01184e]\u001b[39m\u001b[37m SparseArrays \u001b[39m\n",
      " \u001b[90m [10745b16]\u001b[39m\u001b[37m Statistics \u001b[39m\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"../..\")\n",
    "Pkg.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unconstrained optimization \n",
    "\n",
    "* Problem\n",
    "$$\n",
    "\\min_{\\mathbf{x}\\in \\mathbb{R}^d} f(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "* First-order optimality condition\n",
    "$$\n",
    "\\nabla f(\\mathbf{x}^\\star) = 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's method\n",
    "\n",
    "* Newton's method was originally developed by, not surprisingly, Isaac Newton, and further refined by [Joseph Raphson](https://en.wikipedia.org/wiki/Joseph_Raphson), to find roots of nonlinear equations\n",
    "$g(x) = 0$:\n",
    "$$\n",
    "    x^{(t+1)} = x^{(t)} - \\frac{g(x^{(t)}}{g'(x^{(t)})}.\n",
    "$$\n",
    "\n",
    "* In optimization, we may solve the nonlinear equation $\\nabla f(\\mathbf{x}) = 0$ by Newton-Raphson, i.e., setting $g(\\mathbf{x})=\\nabla f(\\mathbf{x})$.\n",
    "\n",
    "\n",
    "* Newton's method, a.k.a. **Newton-Raphson method**, is considered the gold standard for its fast (quadratic) convergence\n",
    "$$\n",
    "\t\\frac{\\|\\mathbf{x}^{(t+1)} - \\mathbf{x}^*\\|}{\\|\\mathbf{x}^{(t)} - \\mathbf{x}^*\\|^2} \\to \\text{constant}.\n",
    "$$\n",
    "In words, the estimate gets accurate by two decimal digits per iteration.\n",
    "\n",
    "* Idea: iterative quadratic approximation. \n",
    "\n",
    "* Second-order Taylor expansion of the objective function around the current iterate $\\mathbf{x}^{(t)}$\n",
    "$$\n",
    "\tf(\\mathbf{x}) \\approx f(\\mathbf{x}^{(t)}) + \\nabla f(\\mathbf{x}^{(t)})^T (\\mathbf{x} - \\mathbf{x}^{(t)}) + \\frac {1}{2} (\\mathbf{x} - \\mathbf{x}^{(t)})^T [\\nabla^2 f(\\mathbf{x}^{(t)})] (\\mathbf{x} - \\mathbf{x}^{(t)})\n",
    "$$\n",
    "and then minimize the quadratic approximation.\n",
    "\n",
    "* To maximize the quadratic appriximation function, we equate its gradient to zero\n",
    "$$\n",
    "\t\\nabla f(\\mathbf{x}^{(t)}) + [\\nabla^2 f(\\mathbf{x}^{(t)})] (\\mathbf{x} - \\mathbf{x}^{(t)}) = \\mathbf{0},\n",
    "$$\n",
    "which suggests the next iterate\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\mathbf{x}^{(t+1)} &=& \\mathbf{x}^{(t)} - [\\nabla^2 f(\\mathbf{x}^{(t)})]^{-1} \\nabla f(\\mathbf{x}^{(t)}),\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "a complete analogue of the univariate Newton-Raphson for solving $g(x)=0$.\n",
    "\n",
    "We call this **naive Newton's method**.\n",
    "\n",
    "* **Stability issue**: naive Newton's iterate is **not** guaranteed to be a descent algorithm, i.e., that ensures $f(\\mathbf{x}^{(t+1)} \\le f(\\mathbf{x}^{(t)})$. It's equally happy to head uphill or downhill. Following example shows that the Newton iterate converges to a local maximum, converges to a local minimum, or diverges depending on starting points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Saved animation to \n",
      "│   fn = /Users/jhwon/Dropbox/class/M1399.000200/2020/lectures/22-newton/newton_sine_1.gif\n",
      "└ @ Plots /Users/jhwon/.julia/packages/Plots/uCh2y/src/animation.jl:104\n",
      "┌ Info: Saved animation to \n",
      "│   fn = /Users/jhwon/Dropbox/class/M1399.000200/2020/lectures/22-newton/newton_sine_2.gif\n",
      "└ @ Plots /Users/jhwon/.julia/packages/Plots/uCh2y/src/animation.jl:104\n",
      "┌ Info: Saved animation to \n",
      "│   fn = /Users/jhwon/Dropbox/class/M1399.000200/2020/lectures/22-newton/newton_sine_3.gif\n",
      "└ @ Plots /Users/jhwon/.julia/packages/Plots/uCh2y/src/animation.jl:104\n"
     ]
    }
   ],
   "source": [
    "using Plots; gr()\n",
    "using ForwardDiff  # for symbolic differentiation\n",
    "\n",
    "f(x) = sin(x)\n",
    "df = x -> ForwardDiff.derivative(f, x) # gradient\n",
    "d2f = x -> ForwardDiff.derivative(df, x) # hessian\n",
    "xarray = [2.0, 2.75, 4.0] # start point: 2.0 (local maximum), 2.75 (diverge), 4.0 (local minimum)\n",
    "for i=1:length(xarray)\n",
    "    x = xarray[i]\n",
    "    titletext = \"Starting point: $x\"\n",
    "    anim = @animate for iter in 0:10\n",
    "        iter > 0 && (x = x - d2f(x) \\ df(x))\n",
    "        p = Plots.plot(f, 0, 2π, xlim=(0, 2π), ylim=(-1.1, 1.1), legend=nothing, title=titletext)\n",
    "        Plots.plot!(p, [x], [f(x)], shape=:circle)\n",
    "        Plots.annotate!(p, x, f(x), text(\"x($iter)\", :right))\n",
    "    end\n",
    "    gif(anim, string(\"./newton_sine_\", i, \".gif\"), fps = 1);\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./newton_sine_1.gif\">\n",
    "\n",
    "<img src=\"./newton_sine_2.gif\">\n",
    "\n",
    "<img src=\"./newton_sine_3.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A remedy for the instability issue:\n",
    "    1. approximate $\\nabla^2 f(\\mathbf{x}^{(t)})$ by a positive definite $\\mathbf{H}^{(t)}$ (if it's not), **and** \n",
    "    2. line search (backtracking) to ensure the descent property.   \n",
    "\n",
    "* Why insist on a _positive definite_ approximation of Hessian? By first-order Taylor expansion,\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "    & & f(\\mathbf{x}^{(t)} + s \\Delta \\mathbf{x}^{(t)}) - f(\\mathbf{x}^{(t)}) \\\\\n",
    "    &=& \\nabla f(\\mathbf{x}^{(t)})^T s \\Delta \\mathbf{x}^{(t)} + o(s) \\\\\n",
    "    &=& - s \\cdot \\nabla f(\\mathbf{x}^{(t)})^T [\\mathbf{H}^{(t)}]^{-1} \\nabla f(\\mathbf{x}^{(t)}) + o(s).\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "For $s$ sufficiently small, $f(\\mathbf{x}^{(t)} + s \\Delta \\mathbf{x}^{(t)}) - f(\\mathbf{x}^{(t)})$ is strictly negative if $\\mathbf{H}^{(t)}$ is positive definite. The quantity $\\{\\nabla f(\\mathbf{x}^{(t)})^T [\\mathbf{H}^{(t)}]^{-1} \\nabla f(\\mathbf{x}^{(t)})\\}^{1/2}$ is termed the **Newton decrement**.\n",
    "\n",
    "* In summary, a **practical Newton-type algorithm** iterates according to\n",
    "$$\n",
    "\t\\boxed{ \\mathbf{x}^{(t+1)} = \\mathbf{x}^{(t)} - s [\\mathbf{H}^{(t)}]^{-1} \\nabla f(\\mathbf{x}^{(t)}) \n",
    "\t= \\mathbf{x}^{(t)} + s \\Delta \\mathbf{x}^{(t)} }\n",
    "$$\n",
    "where $\\mathbf{H}^{(t)}$ is a positive definite approximation to $\\nabla^2 f(\\mathbf{x}^{(t)})$ and $s$ is a step size.\n",
    "\n",
    "* For strictly convex $f$, $\\nabla^2 f(\\mathbf{x}^{(t)})$ is always positive definite. In this case, the above algorithm is called **damped Newton**. However, line search is still needed (at least for a finite number of times) to guarantee convergence.\n",
    "\n",
    "* Line search strategy: step-halving ($s=1,1/2,\\ldots$), golden section search, cubic interpolation, Amijo rule, ... Note the **Newton direction**  \n",
    "$$\n",
    "\\Delta \\mathbf{x}^{(t)} = [\\mathbf{H}^{(t)}]^{-1} \\nabla f(\\mathbf{x}^{(t)})\n",
    "$$\n",
    "only needs to be calculated once. Cost of line search mainly lies in objective function evaluation.\n",
    "```Julia\n",
    "    # Backtracking line search\n",
    "    # given: descent direction ∆x, x ∈ domf, α ∈ (0,0.5), β ∈ (0,1).\n",
    "    t = 1.0\n",
    "    while f(x + t∆x) > f(x) + α * t * ∇f(x)'∆x\n",
    "        t *= β\n",
    "    end\n",
    "```\n",
    "\n",
    "<img src=\"https://jfukuyama.github.io/teaching/stat710/notes/backtracking-image.png\" width=400>\n",
    "\n",
    "> The lower dashed line shows the linear extrapolation of $f$, and the upper dashed line has a slope a factor of α smaller. The backtracking condition is that $f$ lies below the upper dashed line, i.e., $0 \\le t \\le t_0$.  -- Boyd & Vandenberghe Sect 9.2\n",
    "\n",
    "* How to approximate $\\nabla^2 f(\\mathbf{x})$? More of an art than science. Often requires problem specific analysis. \n",
    "\n",
    "* Taking $\\mathbf{H} = \\mathbf{I}$ leads to the **gradient descent method**.\n",
    "\n",
    "<img src=\"http://trond.hjorteland.com/thesis/img208.gif\" width=\"400\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher scoring\n",
    "\n",
    "* Consider MLE in which $f(\\mathbf{x}) = -\\ell(\\boldsymbol{\\theta})$, where $\\ell(\\boldsymbol{\\theta})$ is the log-likelihood of parameter $\\boldsymbol{\\theta}$.\n",
    "\n",
    "* **Fisher scoring method**: replace $- \\nabla^2 \\ell(\\boldsymbol{\\theta})$ by the expected Fisher information matrix\n",
    "$$\n",
    "\t\\mathbf{I}(\\theta) = \\mathbf{E}[-\\nabla^2\\ell(\\boldsymbol{\\theta})] = \\mathbf{E}[\\nabla \\ell(\\boldsymbol{\\theta}) \\nabla \\ell(\\boldsymbol{\\theta})^T] \\succeq \\mathbf{0},\n",
    "$$\n",
    "which is true under exchangeability of tne expectation and the differentiation.\n",
    "\n",
    "    Therefore we set $\\mathbf{H}^{(t)}=\\mathbf{I}(\\boldsymbol{\\theta}^{(t)})$ and obtain the Fisher scoring algorithm: \n",
    "$$\n",
    "\t\\boxed{ \\boldsymbol{\\theta}^{(t+1)} = \\boldsymbol{\\theta}^{(t)} + s [\\mathbf{I}(\\boldsymbol{\\theta}^{(t)})]^{-1} \\nabla \\ell(\\boldsymbol{\\theta}^{(t)})}.\n",
    "$$\n",
    "\n",
    "* Combined with line search, a descent algorithm can be devised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: logistic regression\n",
    "\n",
    "* Binary data: $y_i \\in \\{0,1\\}$, $\\mathbf{x}_i \\in \\mathbb{R}^{p}$. \n",
    "\n",
    "* Model: $y_i \\sim $Bernoulli$(p_i)$, where\n",
    "\\begin{eqnarray*}\n",
    "\t\\mathbf{E} (y_i) = p_i &=& g^{-1}(\\eta_i) = \\frac{e^{\\eta_i}}{1+ e^{\\eta_i}} \\quad \\text{(mean function, inverse link function)} \\\\\n",
    "\t\\eta_i = \\mathbf{x}_i^T \\beta &=& g(p_i) = \\log \\left( \\frac{p_i}{1-p_i} \\right) \\quad \\text{(logit link function)}.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "\n",
    "* MLE: density\n",
    "\\begin{eqnarray*}\n",
    "\tf(y_i|p_i) &=& p_i^{y_i} (1-p_i)^{1-y_i} \\\\\n",
    "\t&=& e^{y_i \\log p_i + (1-y_i) \\log (1-p_i)} \\\\\n",
    "\t&=& \\exp\\left( y_i \\log \\frac{p_i}{1-p_i} + \\log (1-p_i)\\right).\n",
    "\\end{eqnarray*}\n",
    "\n",
    "\n",
    "* Log likelihood of the data $(y_i,\\mathbf{x}_i)$, $i=1,\\ldots,n$, and its derivatives are\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\t\\ell(\\beta) &=& \\sum_{i=1}^n \\left[ y_i \\ln p_i + (1-y_i) \\log (1-p_i) \\right] \\\\\n",
    "\t&=& \\sum_{i=1}^n \\left[ y_i \\mathbf{x}_i^T \\beta - \\log (1 + e^{\\mathbf{x}_i^T \\beta}) \\right] \\\\\n",
    "\t\\nabla \\ell(\\beta) &=& \\sum_{i=1}^n \\left( y_i \\mathbf{x}_i - \\frac{e^{\\mathbf{x}_i^T \\beta}}{1+e^{\\mathbf{x}_i^T \\beta}} \\mathbf{x}_i \\right) \\\\\n",
    "\t&=& \\sum_{i=1}^n (y_i - p_i) \\mathbf{x}_i = \\mathbf{X}^T (\\mathbf{y} - \\mathbf{p})\t\\\\\n",
    "\t- \\nabla^2\\ell(\\beta) &=& \\sum_{i=1}^n p_i(1-p_i) \\mathbf{x}_i \\mathbf{x}_i^T = \\mathbf{X}^T \\mathbf{W} \\mathbf{X}, \\quad\n",
    "\t\\text{where } \\mathbf{W} &=& \\text{diag}(w_1,\\ldots,w_n), w_i = p_i (1-p_i) \\\\\n",
    "\t\\mathbf{I}(\\beta) &=& \\mathbf{E} [- \\nabla^2\\ell(\\beta)] = \\mathbf{X}^T \\mathbf{W} \\mathbf{X} = - \\nabla^2\\ell(\\beta)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "(why the last line?)\n",
    "\n",
    "* Therefore for this problem **Newton's method == Fisher scoring**: \n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\beta^{(t+1)} &=& \\beta^{(t)} + s[-\\nabla^2 \\ell(\\beta^{(t)})]^{-1} \\nabla \\ell(\\beta^{(t)})\t\\\\\n",
    "\t&=& \\beta^{(t)} + s(\\mathbf{X}^T \\mathbf{W}^{(t)} \\mathbf{X})^{-1} \\mathbf{X}^T (\\mathbf{y} - \\mathbf{p}^{(t)}) \\\\\n",
    "\t&=& (\\mathbf{X}^T \\mathbf{W}^{(t)} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W}^{(t)} \\left[ \\mathbf{X} \\beta^{(t)} + s(\\mathbf{W}^{(t)})^{-1} (\\mathbf{y} - \\mathbf{p}^{(t)}) \\right] \\\\\n",
    "\t&=& (\\mathbf{X}^T \\mathbf{W}^{(t)} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W}^{(t)} \\mathbf{z}^{(t)},\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\t\\mathbf{z}^{(t)} = \\mathbf{X} \\beta^{(t)} + s(\\mathbf{W}^{(t)})^{-1} (\\mathbf{y} - \\mathbf{p}^{(t)})\n",
    "$$ \n",
    "are the working responses. A Newton iteration is equivalent to solving a weighed least squares problem $\\sum_{i=1}^n w_i (z_i - \\mathbf{x}_i^T \\beta)^2$. Thus the name **IRLS (iteratively re-weighted least squares)**.\n",
    "\n",
    "* Implication: if a weighted least squares solver is at hand, then logistic regression models can be fitted.\n",
    "    - Now we know alternatives -- GP solver or exponential cone solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: GLM \n",
    "\n",
    "Let's consider a more general class of generalized linear models (GLM).\n",
    "\n",
    "#### Exponential families\n",
    "\n",
    "* Random variable $Y$ belongs to an exponential family if the density\n",
    "$$\n",
    "\tp(y|\\eta,\\phi) = \\exp \\left\\{ \\frac{y\\eta - b(\\eta)}{a(\\phi)} + c(y,\\phi) \\right\\}.\n",
    "$$\n",
    "    * $\\eta$: natural parameter.  \n",
    "    * $\\phi>0$: dispersion parameter.  \n",
    "    * Mean: $\\mu= b'(\\eta)$. When $b'(\\cdot)$ is invertible, function $g(\\cdot)=[b']^{-1}(\\cdot)$ is called the canonical link function.\n",
    "    * Variance $\\mathbf{Var}{Y}=b''(\\eta)a(\\phi)$.\n",
    "\n",
    "\n",
    "* For example, if $Y \\sim \\text{Ber}(\\mu)$, then\n",
    "$$\n",
    "p(y|\\eta,\\phi) = \\exp\\left( y \\log \\frac{\\mu}{1-\\mu} + \\log (1-\\mu)\\right).\n",
    "$$\n",
    "Hence\n",
    "$$\n",
    "\\eta = \\log \\frac{\\mu}{1-\\mu}, \\quad\n",
    "\\mu = \\frac{e^{\\eta}}{1+e^{\\eta}},\n",
    "\\quad\n",
    "b(\\eta) = -\\log (1-\\mu) = \\log(1+e^{\\eta})\n",
    "$$\n",
    "Hence\n",
    "$$\n",
    "b'(\\eta) = \\frac{e^{\\eta}}{1+e^{\\eta}} = g^{-1}(\\eta).\n",
    "$$\n",
    "as above.\n",
    "\n",
    "\n",
    "| Family           | Canonical Link                                 | Variance Function |\n",
    "|------------------|------------------------------------------------|-------------------|\n",
    "| Normal (unit variance)           | $\\eta=\\mu$                                     | 1                 |\n",
    "| Poisson          | $\\eta=\\log \\mu$                                | $\\mu$             |\n",
    "| Binomial         | $\\eta=\\log \\left(\\frac{\\mu}{1 - \\mu} \\right)$  | $\\mu (1 - \\mu)$   |\n",
    "| Gamma            | $\\eta = \\mu^{-1}$                              | $\\mu^2$           |\n",
    "| Inverse Gaussian | $\\eta = \\mu^{-2}$                              | $\\mu^3$           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalized linear models\n",
    "\n",
    "GLM models the conditional distribution of $Y$ given predictors $\\mathbf{x}$ through\n",
    "the conditional mean $\\mu = \\mathbf{E}(Y|\\mathbf{x})$ via a strictly increasing link function\n",
    "$$\n",
    "\tg(\\mu) = \\mathbf{x}^T \\beta, \\quad \\mu = g^{-1}(\\mathbf{x}^T\\beta) = b'(\\eta)\n",
    "$$\n",
    "\n",
    "From these relations we have (assuming no overdispertion, i.e., $a(\\phi)\\equiv 1$)\n",
    "$$\n",
    "\\mathbf{x}^T\\beta = g'(\\mu)d\\mu,\n",
    "\\quad\n",
    "d\\mu = b''(\\eta)d\\eta, \n",
    "\\quad\n",
    "b''(\\eta) = \\mathbf{Var}[Y] = \\sigma^2,\n",
    "\\quad\n",
    "d\\eta = \\frac{1}{b''(\\eta)}d\\mu = \\frac{1}{b''(\\eta)g'(\\mu)}\\mathbf{x}^T d\\beta.\n",
    "$$\n",
    "\n",
    "Then\n",
    "\\begin{align*}\n",
    "d\\ell(\\beta) &= yd\\eta - b'(\\eta)d\\eta = yd\\eta - \\mu d\\eta = (y-\\mu)d\\eta \\\\\n",
    "    &= \\frac{(y-\\mu)\\mathbf{x}^T}{b''(\\eta)g'(\\mu)}d\\beta \n",
    "    = \\frac{1}{\\sigma^2}\\frac{(y-\\mu)\\mathbf{x}^T}{g'(\\mu)}d\\beta\n",
    "    = \\nabla\\ell(\\beta)^Td\\beta,\n",
    "\\end{align*}\n",
    "i.e.,\n",
    "$$\n",
    "\\nabla\\ell(\\beta) = \\frac{1}{g'(\\mu)}\\frac{(y-\\mu)\\mathbf{x}}{\\sigma^2},\n",
    "\\quad\n",
    "\\mu = g^{-1}(\\mathbf{x}^T\\beta).\n",
    "$$\n",
    "\n",
    "To compute the Hessian, observe that\n",
    "\\begin{align*}\n",
    "d^2\\eta &= -\\frac{(\\mathbf{x}^Td\\beta)[b'''(\\eta)d\\eta g'(\\mu) + b''(\\eta)g''(\\mu)d\\mu]}{[b''(\\eta)g'(\\mu)]^2} \\\\\n",
    "    &= -\\frac{(\\mathbf{x}^Td\\beta)[b'''(\\eta)/b''(\\eta)(\\mathbf{x}^T d\\beta) g'(\\mu) + b''(\\eta)g''(\\mu)/g'(\\mu)(\\mathbf{x}^Td\\beta)]}{[b''(\\eta)g'(\\mu)]^2} \\\\\n",
    "    &= -\\frac{b'''(\\eta)g'(\\mu)+[b''(\\eta)]^2g''(\\mu)}{[b''(\\eta)g'(\\mu)]^3}d\\beta^T\\mathbf{x}\\mathbf{x}^Td\\beta.\n",
    "\\end{align*}\n",
    "\n",
    "Then\n",
    "\\begin{align*}\n",
    "d^2\\ell(\\beta) &= -d\\mu d\\eta + (y-\\mu)d^2\\eta \\\\\n",
    "    &= -\\frac{b''(\\eta)}{[b''(\\eta)g'(\\mu)]^2}d\\beta^T\\mathbf{x}\\mathbf{x}^Td\\beta  \n",
    "        -(y-\\mu)\\frac{b'''(\\eta)g'(\\mu)+[b''(\\eta)]^2g''(\\mu)}{[b''(\\eta)g'(\\mu)]^3}d\\beta^T\\mathbf{x}\\mathbf{x}^Td\\beta\n",
    "\\end{align*}\n",
    "yielding\n",
    "$$\n",
    "\\nabla^2\\ell(\\beta) = -\\frac{1}{b''(\\eta)[g'(\\mu)]^2}\\mathbf{x}\\mathbf{x}^T\n",
    "        -(y-\\mu)\\frac{b'''(\\eta)/[b''(\\eta)g'(\\mu)]^2+g''(\\mu)/[g'(\\mu)]^3}{b''(\\eta)}\\mathbf{x}\\mathbf{x}^T.\n",
    "$$\n",
    "\n",
    "To sum up, for $n$ samples we have\n",
    "\n",
    "* Score, Hessian, information\n",
    "\n",
    "\\begin{eqnarray*}\n",
    " \t\\nabla\\ell(\\beta) &=& \\sum_{i=1}^n \\frac{(y_i-\\mu_i) [1/g'(\\mu_i)]}{\\sigma^2} \\mathbf{x}_i, \\quad \\mu_i = \\mathbf{x}_i^T\\beta, \\\\\n",
    "\t- \\nabla^2 \\ell(\\beta) &=& \\sum_{i=1}^n \\frac{[1/g'(\\mu_i)]^2}{\\sigma^2} \\mathbf{x}_i \\mathbf{x}_i^T - \\sum_{i=1}^n \\frac{(y_i - \\mu_i)[b'''(\\eta_i)/[b''(\\eta_i)g'(\\mu_i)]^2+g''(\\mu_i)/[g'(\\mu_i)]^3]}{\\sigma^2} \\mathbf{x}_i \\mathbf{x}_i^T, \\quad \\eta_i = [b']^{-1}(\\eta),\t\\\\\n",
    "\t\\mathbf{I}(\\beta) &=& \\mathbf{E} [- \\nabla^2 \\ell(\\beta)] = \\sum_{i=1}^n \\frac{[1/g'(\\mu_i)]^2}{\\sigma^2} \\mathbf{x}_i \\mathbf{x}_i^T = \\mathbf{X}^T \\mathbf{W} \\mathbf{X}.\n",
    "\\end{eqnarray*}    \n",
    "\n",
    "\n",
    "* Fisher scoring method:\n",
    "$$\n",
    " \t\\beta^{(t+1)} = \\beta^{(t)} + s [\\mathbf{I}(\\beta^{(t)})]^{-1} \\nabla \\ell(\\beta^{(t)})\n",
    "$$\n",
    "IRLS with weights $w_i = [1/g'(\\mu_i)]^2/\\sigma^2$ and some working responses $z_i$.\n",
    "\n",
    "* For *canonical link*, $\\mathbf{x}^T\\beta = g(\\mu) =[b']^{-1}(\\mu) = \\eta$. The second term of Hessian vanishes because $d\\eta=\\mathbf{x}^Td\\beta$ and $d^2\\eta=0$. The Hessian coincides with Fisher information matrix. **Fisher scoring == Newton's method**. Hence MLE is a *convex* optimization problem.\n",
    "\n",
    " \n",
    "* Non-canonical link, **Fisher scoring != Newton's method**, and MLE is a *non-convex* optimization problem.\n",
    "\n",
    "  Example: Probit regression (binary response with probit link).\n",
    "\\begin{eqnarray*}\n",
    "    y_i &\\sim& \\text{Ber}(p_i) \\\\\n",
    "    p_i &=& \\Phi(\\mathbf{x}_i^T \\beta)  \\\\\n",
    "    \\eta_i &=& \\log\\left(\\frac{p_i}{1-p_i}\\right) \\neq \\mathbf{x}_i^T \\beta = \\Phi^{-1}(p_i).\n",
    "\\end{eqnarray*}\n",
    "  where $\\Phi(\\cdot)$ is the cdf of a standard normal.\n",
    " \n",
    "* Julia, R, and Matlab implement the Fisher scoring method, aka IRLS, for GLMs.\n",
    "    * [GLM.jl](https://github.com/JuliaStats/GLM.jl) package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear regression - Gauss-Newton method\n",
    "\n",
    "* Now we finally get to the problem Gauss faced in 1801!  \n",
    "Relocate the dwarf planet Ceres <https://en.wikipedia.org/wiki/Ceres_(dwarf_planet)> by fitting 24 observations to a 6-parameter (nonlinear) orbit.\n",
    "    - In 1801, Jan 1 -- Feb 11 (41 days), astronomer Piazzi discovered Ceres, which was lost behind the Sun after observing its orbit 24 times.\n",
    "    - Aug -- Sep, futile search by top astronomers; Laplace claimed it unsolvable.\n",
    "    - Oct -- Nov, Gauss did calculations by method of least squares, sent his results to astronomer von Zach.\n",
    "    - Dec 31, von Zach relocated Ceres according to Gauss’ calculation.\n",
    "\n",
    "* Nonlinear least squares (curve fitting): \n",
    "$$\n",
    "\t\\text{minimize} \\,\\, f(\\beta) = \\frac{1}{2} \\sum_{i=1}^n [y_i - \\mu_i(\\mathbf{x}_i, \\beta)]^2\n",
    "$$\n",
    "For example, $y_i =$ dry weight of onion and $x_i=$ growth time, and we want to fit a 3-parameter growth curve\n",
    "$$\n",
    "\t\\mu(x, \\beta_1,\\beta_2,\\beta_3) = \\frac{\\beta_3}{1 + \\exp(-\\beta_1 - \\beta_2 x)}.\n",
    "$$\n",
    "\n",
    "<img src=\"https://cdn.xlstat.com/img/tutorials/nlin5.gif\" width=\"300\" align=\"center\"/>\n",
    "\n",
    "* \"Score\" and \"information matrices\"\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\nabla f(\\beta) &=& - \\sum_{i=1}^n [y_i - \\mu_i(\\beta)] \\nabla \\mu_i(\\beta) \\\\\n",
    "\t\\nabla^2 f(\\beta) &=& \\sum_{i=1}^n \\nabla \\mu_i(\\beta) \\nabla \\mu_i(\\beta)^T - \\sum_{i=1}^n [y_i - \\mu_i(\\beta)] \\nabla^2 \\mu_i(\\beta) \\\\\n",
    "\t\\mathbf{I}(\\beta) &=& \\sum_{i=1}^n \\nabla \\mu_i(\\beta) \\nabla \\mu_i(\\beta)^T = \\mathbf{J}(\\beta)^T \\mathbf{J}(\\beta),\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "where $\\mathbf{J}(\\beta)^T = [\\nabla \\mu_1(\\beta), \\ldots, \\nabla \\mu_n(\\beta)] \\in \\mathbb{R}^{p \\times n}$.\n",
    "\n",
    "* **Gauss-Newton** (= \"Fisher scoring method\") uses $\\mathbf{I}(\\beta)$, which is always positive semidefinite.\n",
    "$$\n",
    "\t\\boxed{ \\beta^{(t+1)} = \\beta^{(t)} - s [\\mathbf{I} (\\beta^{(t)})]^{-1} \\nabla f(\\beta^{(t)}) }\n",
    "$$\n",
    "based on the rationale that either the residuals $y_i − \\mu_i(\\beta)$ are small or the regression functions $\\mu_i(\\beta)$ are nearly linear.\n",
    "\n",
    "* **Levenberg-Marquardt** method or **damped least squares algorithm (DLS)**, adds a ridge term to the approximate Hessian\n",
    "$$\n",
    "\t\\boxed{ \\beta^{(t+1)} = \\beta^{(t)} - s [\\mathbf{I} (\\beta^{(t)}) + \\tau \\mathbf{Id}_p]^{-1} \\nabla f(\\beta^{(t)}) }\n",
    "$$\n",
    "bridging between Gauss-Newton and steepest descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* Boyd and Vandenberghe, 2004. Convex Optimization. Cambridge University Press. (Ch. 9)\n",
    "\n",
    "* Lange, K., 2010. Numerical analysis for statisticians. Springer Science & Business Media. (Ch. 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgment\n",
    "\n",
    "Many parts of this lecture note is based on [Dr. Hua Zhou](http://hua-zhou.github.io)'s 2019 Spring Statistical Computing course notes available at <http://hua-zhou.github.io/teaching/biostatm280-2019spring/index.html>."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.4.0",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "153px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "441.3333435058594px",
    "left": "0px",
    "right": "903.3333129882813px",
    "top": "140.6666717529297px",
    "width": "166px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
