{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"../..\")\n",
    "Pkg.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer arithmetics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Units of computer storage\n",
    "\n",
    "Humans use decimal digits (why?)\\\n",
    "Computers use binary digits (why?)\n",
    "\n",
    "* *Bit* = binary digit (coined by statistician [John Tukey](https://en.wikipedia.org/wiki/Bit#History)).  \n",
    "* *byte* = 8 bits.\n",
    "* KB = kilobyte = $10^3$ bytes; KiB = kibibyte = $2^{10}$ bytes.  \n",
    "* MB = megabyte = $10^6$ bytes; MiB = mebibyte = $2^{20}$ bytes.\n",
    "* GB = gigabyte = $10^9$ bytes. Typical RAM size.  \n",
    "* TB = terabyte = $10^{12}$ bytes. Typical hard drive size. Size of NYSE each trading session.    \n",
    "* PB = petabyte = $10^{15}$ bytes.  \n",
    "* EB = exabyte = $10^{18}$ bytes. Size of all healthcare data in 2011 is ~150 EB.    \n",
    "* ZB = zetabyte = $10^{21}$ bytes. \n",
    "\n",
    "Julia function `Base.summarysize` shows the amount of memory (in bytes) used by an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rand(100, 100)\n",
    "Base.summarysize(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`varinfo()` function prints all variables in workspace and their sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varinfo() # similar to Matlab whos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage of Characters\n",
    "\n",
    "* Plain text files are stored in the form of characters: `.jl`, `.r`, `.c`, `.cpp`, `.ipynb`, `.html`, `.tex`, ...  \n",
    "* ASCII (American Code for Information Interchange): 7 bits, only $2^7=128$ characters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integers 0, 1, ..., 127 and corresponding ascii character\n",
    "[0:127 Char.(0:127)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Extended ASCII: 8 bits, $2^8=256$ characters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integers 128, 129, ..., 255 and corresponding extended ascii character\n",
    "# show(STDOUT, \"text/plain\", [128:255 Char.(128:255)])\n",
    "[128:255 Char.(128:255)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Unicode: UTF-8, UTF-16 and UTF-32 support many more characters including foreign characters; last 7 digits conform to ASCII. \n",
    "\n",
    "* [UTF-8](https://en.wikipedia.org/wiki/UTF-8) is the current dominant character encoding on internet.  \n",
    "\n",
    "<img src=\"./unicode.png\" width=\"800\" align=\"center\"/>\n",
    "\n",
    "Source: [Google Blog](https://googleblog.blogspot.com/2012/02/unicode-over-60-percent-of-web.html)\n",
    "\n",
    "* Julia supports the full range of UTF-8 characters. You can type many Unicode math symbols by typing the backslashed LaTeX symbol name followed by tab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \\beta-<tab>\n",
    "β = 0.0\n",
    "# \\beta-<tab>-\\hat-<tab>\n",
    "β̂ = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For a table of unicode symbols that can be entered via tab completion of LaTeX-like abbreviations: <https://docs.julialang.org/en/v1.1/manual/unicode-input/#Unicode-Input-1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integers: fixed-point number system\n",
    "\n",
    "* Fixed-point number system is a computer model for integers $\\mathbb{Z}$. \n",
    "    - **Remember** that computer memory is finite whereas the cardinality of $\\mathbb{Z}$ is (countably) infinite.\n",
    "    - *Any* representation of numbers in computer *has to be* an approximation.\n",
    "\n",
    "* The number $M$ of bits and method of representing negative numbers vary from system to system. \n",
    "    - The `integer` type in R has $M=32$ (packages such as ‘bit64’support 64 bit integers). \n",
    "        + <https://www.r-bloggers.com/r-in-a-64-bit-world/>\n",
    "    - C has (`unsigned`) `char`, `int`, `short`, `long` (and `long long`), whose sizes depend on the machine.\n",
    "    - Matlab has `(u)int8`, `(u)int16`, `(u)int32`, `(u)int64`.  \n",
    "\n",
    "* Julia has even more integer types: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using GraphRecipes, Plots\n",
    "\n",
    "gr(size=(600, 400))\n",
    "theme(:default)\n",
    "\n",
    "plot(Integer, method=:tree, fontsize=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: [Visualizing Graphs in Julia using Plots and PlotRecipes](http://www.breloff.com/Graphs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsigned integers\n",
    "\n",
    "* Model for $\\mathbb{N} \\cup \\{0\\}$.\n",
    "* For unsigned integers, the range is $[0,2^M-1]$.\n",
    "* Julia functions `typemin(T)` and `typemax(T)` give the lowest and highest representable number of a type `T` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in [UInt8, UInt16, UInt32, UInt64, UInt128]\n",
    "    println(t, '\\t', typemin(t), '\\t', typemax(t))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signed integers\n",
    "\n",
    "* Model of $\\mathbb{Z}$. Can do subtraction.\n",
    "\n",
    "* First bit (\"most significant bit\" or MSB) is the sign bit.  \n",
    "    - `0` for nonnegative numbers\n",
    "    - `1` for negative numbers  \n",
    "    \n",
    "* **Two's complement representation** for negative numbers. \n",
    "    - Set the sign bit to 1  \n",
    "    - Negate (`0`->`1`, `1`->`0`) the remaining bits\n",
    "    - Add to `1` to the result  \n",
    "    - Two's complement representation of a negative integer $x$ is the same as the unsigned integer $2^M - x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show typeof(5)\n",
    "@show bitstring(5)\n",
    "@show bitstring(-5)\n",
    "@show bitstring(UInt64(Int128(2)^64 - 5)) == bitstring(-5)\n",
    "@show bitstring(2 * 5) # shift bits of 5 to the left\n",
    "@show bitstring(2 * -5); # shift bits of -5 to left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Two's complement representation respects modular arithmetic nicely.  \n",
    "    Addition of any two signed integers are just bitwise addition, possibly modulo $2^M$\n",
    "    - $M=4$ case:\n",
    "    \n",
    "<img src=\"http://users.dickinson.edu/~braught/courses/cs251f02/classes/images/twosCompWheel.png\" width=\"400\" align=\"center\"/>    \n",
    "\n",
    "Source: [Signed Binary Numbers, Subtraction and Overflow](http://users.dickinson.edu/~braught/courses/cs251f02/classes/notes07.html) by Grant Braught"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Range** of representable integers by $M$-bit **signed integer** is $[-2^{M-1},2^{M-1}-1]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typemin(Int64), typemax(Int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for T in [Int8, Int16, Int32, Int64, Int128]\n",
    "    println(T, '\\t', typemin(T), '\\t', typemax(T))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `BigInt`\n",
    "\n",
    "Julia `BigInt` type is arbitrary precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show typemax(Int128)\n",
    "@show typemax(Int128) + 1 # overflow!\n",
    "@show BigInt(typemax(Int128)) + 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overflow and underflow for integer arithmetic\n",
    "\n",
    "R reports `NA` for integer overflow and underflow.  \n",
    "**Julia outputs the result according to modular arithmetic.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show typemax(Int32)\n",
    "@show typemax(Int32) + Int32(1); # modular arithmetics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using RCall\n",
    "# The largest integer R can hold\n",
    "R\"\"\"\n",
    ".Machine$integer.max \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R\"\"\"\n",
    "M <- 32\n",
    "big <- 2^(M-1) - 1\n",
    "as.integer(big)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R\"\"\"\n",
    "as.integer(big+1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real numbers: floating-point number system\n",
    "\n",
    "Floating-point number system is a computer model for the real line $\\mathbb{R}$.\n",
    "\n",
    "* Most computer systems adopt the [IEEE 754 standard](https://en.wikipedia.org/wiki/IEEE_floating_point), established in 1985, for floating-point arithmetics.  \n",
    "For the history, see an [interview with William Kahan](http://www.cs.berkeley.edu/~wkahan/ieee754status/754story.html).\n",
    "\n",
    "* In the scientific notation, a real number is represented as\n",
    "$$\n",
    "\\pm d_1.d_2d_3 \\cdots d_p \\times b^e, \\quad 0 \\le d_i < b.\n",
    "$$\n",
    "Humans use the _base_ $b=10$ and _digits_ $d_i=0, 1, \\dotsc, 9$.\\\n",
    "    In computer, the base is $b=2$ and the digits $d_i$ are 0 or 1.\n",
    "\n",
    "* **Normalized** vs **denormalized** numbers. For example, decimal number 18 is\n",
    "$$ +1.0010 \\times 2^4 \\quad (\\text{normalized})$$\n",
    "or, equivalently,\n",
    "$$ +0.1001 \\times 2^5 \\quad (\\text{denormalized}).$$\n",
    "\n",
    "* In the floating-number system, computer stores \n",
    "    - sign bit  \n",
    "    - the _fraction_ (or _mantissa_, or _significand_) of the **normalized** representation\n",
    "    - the actual exponent _plus_ a bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Julia provides `Float16` (half precision, implemented in software using `Float32`), `Float32` (single precision), `Float64` (double precision), and `BigFloat` (arbitrary precision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using GraphRecipes, Plots\n",
    "\n",
    "#pyplot(size=(800, 600))\n",
    "gr(size=(600, 400))\n",
    "theme(:default)\n",
    "\n",
    "plot(AbstractFloat, method=:tree, fontsize=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Half precision (Float16)\n",
    "\n",
    "<img src=\"./half-precision-numbers.png\" width=\"300\" align=\"center\"/>\n",
    "\n",
    "Source: <https://en.wikipedia.org/wiki/Half-precision_floating-point_format>\n",
    "    \n",
    "- In Julia, `Float16` is the type for half precision numbers.\n",
    "\n",
    "- MSB is the sign bit.  \n",
    "\n",
    "- 10 significand bits (**fraction**=**mantissa**), hence $p=11$ (why?)\n",
    "\n",
    "- 5 exponent bits: $e_{\\max}=15$, $e_{\\min}=-14$, **bias**=15 = $01111_2$ for encoding:\n",
    "    + $e_{\\min} = \\mathbf{00001_2} - 01111_2 = -14_{10}$\n",
    "    + $e_{\\max} = \\mathbf{11110_2} - 01111_2 = 15_{10}$\n",
    "\n",
    "- $e_{\\text{min}}-1$ and $e_{\\text{max}}+1$ are reserved for special numbers.  \n",
    "\n",
    "- range of **magnitude**: $10^{\\pm 4}$ in decimal because $\\log_{10} (2^{15}) \\approx 4$.  \n",
    "\n",
    "- **Precision**: $\\log_{10}2^{11} \\approx 3.311$ decimal digits. \n",
    "\n",
    "$$\n",
    "(value) = (-1)^{b_{15}}\\times 2^{(\\sum_{j=1}^5 b_{15-j}2^{5-j}) - 15} \\times \\left( 1 + \\sum_{i=1}^{10}\\frac{b_{10-i}}{2^i}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Half precision:\")\n",
    "@show bitstring(Float16(5)) # 5 in half precision\n",
    "@show bitstring(Float16(-5)); # -5 in half precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single precision (Float32)\n",
    "\n",
    "<img src=\"./single-precision-numbers.png\" width=\"600\" align=\"center\"/>\n",
    "\n",
    "Source: <https://en.wikipedia.org/wiki/Single-precision_floating-point_format>\n",
    "\n",
    "- In Julia, `Float32` is the type for single precision numbers.  \n",
    "\n",
    "- MSB is the sign bit.  \n",
    "\n",
    "- 23 significand bits ($p=24$).  \n",
    "\n",
    "- 8 exponent bits: $e_{\\max}=127$, $e_{\\min}=-126$, **bias**=127.  \n",
    "\n",
    "- $e_{\\text{min}}-1$ and $e_{\\text{max}}+1$ are reserved for special numbers.  \n",
    "\n",
    "- range of **magnitude**: $10^{\\pm 38}$ in decimal because $\\log_{10} (2^{127}) \\approx 38$.\n",
    "\n",
    "- **precision**: $\\log_{10}(2^{24}) \\approx 7.225$ decimal digits.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Single precision:\")\n",
    "@show bitstring(Float32(5)) # 5 in single precision\n",
    "@show bitstring(Float32(-5)); # -5 in single precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double precision (Float64)\n",
    "\n",
    "<img src=\"./double-precision-numbers.png\" width=\"600\" align=\"center\"/>\n",
    "\n",
    "Source: <https://en.wikipedia.org/wiki/Double-precision_floating-point_format>\n",
    "\n",
    "- Double precision (64 bits = 8 bytes) numbers are the dominant data type in scientific computing.\n",
    "      \n",
    "- In Julia, `Float64` is the type for double precision numbers.    \n",
    "\n",
    "- MSB is the sign bit.  \n",
    "\n",
    "- 52 significand bits ($p=53$).\n",
    "\n",
    "- 11 exponent bits: $e_{\\max}=1023$, $e_{\\min}=-1022$, **bias**=1023.  \n",
    "\n",
    "- $e_{\\text{min}}-1$ and $e_{\\text{max}}+1$ are reserved for special numbers.  \n",
    "\n",
    "- range of **magnitude**: $10^{\\pm 308}$ in decimal because $\\log_{10} (2^{1023}) \\approx 308$.  \n",
    "\n",
    "- **precision** to the $\\log_{10}(2^{53}) \\approx 15.95$ decimal point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Double precision:\")\n",
    "@show bitstring(Float64(5)) # 5 in double precision\n",
    "@show bitstring(Float64(-5)); # -5 in double precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special floating-point numbers\n",
    "\n",
    "- Exponent $e_{\\max}+1$ plus a zero mantissa means $\\pm \\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show bitstring(Inf) # Inf in double precision\n",
    "@show bitstring(-Inf); # -Inf in double precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exponent $e_{\\max}+1$ plus a nonzero mantissa means `NaN`. `NaN` could be produced from `0 / 0`, `0 * Inf`, ...  \n",
    "\n",
    "- In general `NaN ≠ NaN` bitwise. Test whether a number is `NaN` by `isnan` function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show bitstring(0 / 0) # NaN\n",
    "@show bitstring(0 * Inf); # NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exponent $e_{\\min}-1$ with a zero mantissa represents the real number 0 (\"exact zero\").  \n",
    "    + Why do we need an exact zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show bitstring(0.0); # 0 in double precision "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exponent $e_{\\min}-1$ with a nonzero mantissa are for numbers less than $b^{e_{\\min}}$.  \n",
    "    Numbers are _denormalized_ in the range $(0,b^{e_{\\min}})$ -- **gradual underflow**. \n",
    "- For example, in half-precision, $e_{\\min}=-14$ but $2^{-24}$ is represented by $0.0000000001_2 \\times 2^{-14}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show Float16(2^(-14))  # emin=-14\n",
    "@show bitstring(Float16(2^(-14)));\n",
    "@show Float16(2^(-24))  # emin=-14\n",
    "@show bitstring(Float16(2^(-24))); # denormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show nextfloat(Float16(0.0)) # next representable number\n",
    "@show bitstring(nextfloat(Float16(0.0))); # denormalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rounding\n",
    "\n",
    "* Rounding is necessary whenever a number has more than $p$ significand bits. Most computer systems use the default IEEE 754 _round to nearest_ mode (also called _ties to even_ mode). Julia offers several [rounding modes](https://docs.julialang.org/en/v1/base/math/#Base.Rounding.RoundingMode), the default being [`RoundNearest`](https://docs.julialang.org/en/v1/base/math/#Base.Rounding.RoundNearest). \n",
    "\n",
    "* \"*Round to nearest, ties to even*\" rule: rounds to the nearest value; if the number falls midway, it is rounded to the nearest value with an even least significant digit (default for IEEE 754 binary floating point numbers)\n",
    "\n",
    "* For example, the number 1/10 cannot be represented accurately as a (binary) floating point number:\n",
    "$$ 0.1 = 1.10011001\\dotsc_2 \\times 2^{-4} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show bitstring(0.1);  # double precision Float64\n",
    "@show bitstring(0.1f0); # single precision Float32, 1001 gets rounded to 101(0)\n",
    "@show bitstring(Float16(0.1)); # half precision Float16, 1001 gets rounded to 101(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors\n",
    "\n",
    "Rounding (more fundamentally, finite precision) incurs errors in floating porint computation. If a real number $x$ is represented by a floating point number $[x]$, then\n",
    "\n",
    "* Absolute error\n",
    "$$\n",
    "    | [x] - x |\n",
    "$$\n",
    "\n",
    "* Relative error\n",
    "$$\n",
    "    \\frac{| [x] - x |}{|x|}\n",
    "$$\n",
    "(if $x \\neq 0$).\n",
    "\n",
    "Of course, we want to ensure that the error after a computation is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine epsilons\n",
    "\n",
    "- Floating-point numbers do not occur uniformly over the real number line\n",
    "    <img src=\"http://www.volkerschatz.com/science/pics/fltscale-wh.png\" width=\"700\" align=\"center\"/>\n",
    "    \n",
    "    Source: [What you never wanted to know about floating point but will be forced to find out](http://www.volkerschatz.com/science/float.html)\n",
    "    \n",
    "- Same number of representible numbers in $(2^i, 2^{i+1}]$ as in $(2^{i+1}, 2^{i+2}]$. Within an interval, they are uniformly distributed.\n",
    "    \n",
    "- **Machine epsilons** are the spacings of numbers around 1: \n",
    "    + $\\epsilon_{\\max}$ = (smallest positive floating point number that added to 1 will give a result different from 1) = $\\frac{1}{2^p} + \\frac{1}{2^{2p-1}}$\n",
    "    + $\\epsilon_{\\min}$ = (smallest positive floating point number that subtracted from 1 will give a result different from 1) = $\\frac{1}{2^{p+1}} + \\frac{1}{2^{2p}}$.\n",
    "    \n",
    "    <img src=\"./machine_epsilons.png\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "Source: *Computational Statistics*, James Gentle, Springer, New York, 2009.\n",
    "    \n",
    "- Any real number in the interval $\\left[1 - \\frac{1}{2^{p+1}}, 1 + \\frac{1}{2^p}\\right]$ is represented by a floating point number $1 = 1.00\\dotsc 0_2 \\times 2^0$ (assuming the \"ties to even\" rule: consider $p=2$ case).\n",
    "\n",
    "- Adding $\\frac{1}{2^p}$ to 1 won't reach the next representable floating point number  $1.00\\dotsc 01_2 \\times 2^0 = 1 + \\frac{1}{2^{p-1}}$. Hence $\\epsilon_{\\max} > \\frac{1}{2^p} = 1.00\\dotsc 0_2 \\times 2^{-p}$.\n",
    "\n",
    "- Adding the floating point number next to $\\frac{1}{2^p} = 1.00\\dotsc 0_2 \\times 2^{-p}$ to 1 WILL result in $1.00\\dotsc 01_2 \\times 2^0 = 1 + \\frac{1}{2^{p-1}}$, hence $\\epsilon_{\\max} = 1.00\\dotsc 01_2 \\times 2^{-p} = \\frac{1}{2^p} + \\frac{1}{2^{p+p-1}}$.\n",
    "\n",
    "- Subtracting $\\frac{1}{2^{p+1}}$ from 1 results in $1-\\frac{1}{2^{p+1}} = \\frac{1}{2} + \\frac{1}{2^2} + \\dotsb + \\frac{1}{2^p} + \\frac{1}{2^{p+1}}$, which is represented by the floating point number $1.00\\dotsc 0_2 \\times 2^{0} = 1$ by the \"ties to even\" rule. Hence $\\epsilon_{\\min} > \\frac{1}{2^{p+1}}$.\n",
    "\n",
    "- The smallest positive floating point number larger than $\\frac{1}{2^{p+1}}$ is $\\frac{1}{2^{p+1}} + \\frac{1}{2^{2p}}=1.00\\dotsc 1_2 \\times 2^{-p-1}$. Thus $\\epsilon_{\\min} = \\frac{1}{2^{p+1}} + \\frac{1}{2^{2p}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine precision\n",
    "\n",
    "* Machine epsilon is often called the machine precision.\n",
    "\n",
    "* If a positive real number $x \\in \\mathbb{R}$ is represented by $[x]$ in the floating point arithmetic, then \n",
    "$$\n",
    "    [x] = \\left( 1 + \\sum_{i=1}^{p-1}\\frac{b_{i+1}}{2^i}\\right) \\times 2^e.\n",
    "$$\n",
    "Thus $x - \\frac{2^e}{2^p} < [x] \\le x + \\frac{2^e}{2^p}$, \n",
    "and\n",
    "$$\n",
    "    \\begin{split}\n",
    "    \\frac{| x - [x] |}{|x|} &\\le \\frac{2^e}{2^p|x|} \\le \\frac{2^e}{2^p}\\frac{1}{[x]-2^e/2^p} \\\\\n",
    "                            &\\le \\frac{2^e}{2^p}\\frac{1}{2^e(1-1/2^p)}  \\quad (\\because [x] \\ge 2^e) \\\\\n",
    "                            &\\le \\frac{2^e}{2^p}\\frac{1}{2^e}(1 + \\frac{1}{2^{p-1}}) \\\\\n",
    "                            &= \\frac{1}{2^p} + \\frac{1}{2^{2p-1}} = \\epsilon_{\\max}.\n",
    "    \\end{split}\n",
    "$$\n",
    "That is, the **relative error** of the floating point representation $[x]$ of real number $x$ is bounded by $\\epsilon_{\\max}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show 2^(-53) + 2^(-105);   # epsilon_max for Float64\n",
    "@show 1.0 + 2^(-53);\n",
    "@show 1.0 + (2^(-53) + 2^(-105));\n",
    "@show 1.0 + 2^(-53) + 2^(-105);  # why is the result?  See \"Catastrophic cancellation\"\n",
    "\n",
    "@show Float32(2^(-24) + 2^(-47)); # epsilon_max for Float32\n",
    "@show 1.0f0 + Float32(2^(-24));\n",
    "@show 1.0f0 + Float32(2^(-24) + 2^(-47));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show 2^(-54) + 2^(-106);  # epsilon_min for Float64\n",
    "@show 1 - (2^(-54) + 2^(-106))\n",
    "@show bitstring(1.0)\n",
    "@show bitstring(1 - (2^(-54) + 2^(-106)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Julia, `eps(x)` gives the distance between consecutive representable floating point values at `x`, i.e.,\n",
    "```Julia\n",
    "eps(x) == max(x-prevfloat(x), nextfloat(x)-x)\n",
    "```\n",
    "which is roughly twice the $\\epsilon_{\\max}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show eps(Float32)  # machine epsilon for a floating point type\n",
    "@show eps(Float64)  # same as eps()\n",
    "# eps(x) is the spacing after x\n",
    "@show eps(100.0)\n",
    "@show eps(0.0)\n",
    "# nextfloat(x) and prevfloat(x) give the neighbors of x\n",
    "@show x = 1.25f0\n",
    "@show prevfloat(x), x, nextfloat(x)\n",
    "@show bitstring(prevfloat(x)), bitstring(x), bitstring(nextfloat(x));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In R, the variable `.Machine` contains numerical characteristics of the machine. `double.neg.eps` is our $\\epsilon_{\\max}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R\"\"\"\n",
    ".Machine\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overflow and underflow of floating-point number\n",
    "\n",
    "* For double precision, the range is $\\pm 10^{\\pm 308}$. In most situations, underflow (magnitude of result is less than $10^{-308}$) is preferred over overflow (magnitude of result is larger than $10^{+308}$). Overflow produces $\\pm \\inf$. Underflow yields zeros or subnormal numbers. \n",
    "\n",
    "* E.g., the logit link function is\n",
    "$$p = \\frac{\\exp (x^T \\beta)}{1 + \\exp (x^T \\beta)} = \\frac{1}{1+\\exp(- x^T \\beta)}.$$\n",
    "The former expression can easily lead to `Inf / Inf = NaN`, while the latter expression leads to gradual underflow.\n",
    "\n",
    "* `floatmin` and `floatmax` functions gives largest and smallest _non-submormal_ number represented by the given floating point type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for T in [Float16, Float32, Float64]\n",
    "    println(T, '\\t', floatmin(T), '\\t', floatmax(T), '\\t', typemin(T), \n",
    "        '\\t', typemax(T), '\\t', eps(T))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `BigFloat` in Julia offers arbitrary precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show precision(BigFloat)\n",
    "@show floatmin(BigFloat)\n",
    "@show floatmax(BigFloat);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show BigFloat(π); # default precision for BigFloat is 256 bits\n",
    "# set precision to 1024 bits\n",
    "setprecision(BigFloat, 1024) do \n",
    "    @show BigFloat(π)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catastrophic cancellation\n",
    "\n",
    "* **Scenario 1**: Addition or subtraction of two numbers of widely different magnitudes: $a+b$ or $a-b$ where $a \\gg b$ or $a \\ll b$. We lose the precision in the number of smaller magnitude. Consider \n",
    "$$\\begin{eqnarray*}\n",
    "    a &=& x.xxx ... \\times 2^{30} \\\\  \n",
    "    b &=& y.yyy... \\times 2^{-30}\n",
    "\\end{eqnarray*}$$\n",
    "What happens when computer calculates $a+b$? We get $a+b=a$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show a = 2.0^30\n",
    "@show b = 2.0^-30\n",
    "@show a + b == a\n",
    "@show bitstring(a)\n",
    "@show bitstring(a + b);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis: suppose we want to compute $x + y$, $x, y > 0$. Let the relative error of $x$ and $y$ be\n",
    "$$\n",
    "\\delta_x = \\frac{[x] - x}{x},\n",
    "\\quad\n",
    "\\delta_y = \\frac{[y] - y}{y}\n",
    ".\n",
    "$$\n",
    "What the computer actually calculates is $[x] + [y]$, which in turn is represented by $[ [x] + [y] ]$. The relative error of this representation is\n",
    "$$\n",
    "\\delta_{\\text{sum}} = \\frac{[[x]+[y]] - ([x]+[y])}{[x]+[y]}\n",
    ".\n",
    "$$\n",
    "Recall that $|\\delta_x|, |\\delta_y|, |\\delta_{\\text{sum}}| \\le \\epsilon_{\\max}$.\n",
    "\n",
    "We want to find a bound of the relative error of $[[x]+[y]]$ with respect to $x+y$.\n",
    "Since $|[x]+[y]| = |x(1+\\delta_x) + y(1+\\delta_y)| \\le |x+y|(1+\\epsilon_{\\max})$, we have\n",
    "$$\n",
    "\\begin{split}\n",
    "| [[x]+[y]]-(x+y) | &= | [[x]+[y]] - [x] - [y] + [x] - x + [y] - y | \\\\\n",
    "                   &\\le | [[x]+[y]] - [x] - [y] | +  | [x] - x | + | [y] - y | \\\\\n",
    "                   &\\le |\\delta_{\\text{sum}}([x]+[y])| + |\\delta_x x| + |\\delta_y y| \\\\\n",
    "                   &\\le \\epsilon_{\\max}(x+y)(1+\\epsilon_{\\max}) + \\epsilon_{\\max}x + \\epsilon_{\\max}y \\\\\n",
    "                   &\\approx 2\\epsilon_{\\max}(x+y)\n",
    "\\end{split}\n",
    "$$\n",
    "because $\\epsilon_{\\max}^2 \\approx 0$. Thus\n",
    "$$\n",
    "\\frac{| [[x]+[y]]-(x+y) |}{|x+y|} \\le 2\\epsilon_{\\max}\n",
    "$$\n",
    "approximately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Scenario 2**: Subtraction of two nearly equal numbers eliminates significant digits.  $a-b$ where $a \\approx b$. Consider \n",
    "$$\\begin{eqnarray*}\n",
    "    a &=& x.xxxxxxxxxx1ssss  \\\\\n",
    "    b &=& x.xxxxxxxxxx0tttt\n",
    "\\end{eqnarray*}$$\n",
    "The result is $1.vvvvu...u$ where $u$ are unassigned digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1.2345678f0 # rounding\n",
    "@show bitstring(a) # rounding\n",
    "b = 1.2345677f0\n",
    "@show bitstring(b)\n",
    "@show a - b # correct result should be 1f-7\n",
    "@show bitstring(a - b)   # must be 1.0000...0 x 2^(-23)\n",
    "@show Float32(1/2^23);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis: Let\n",
    "$$\n",
    "[x] = 1 + \\sum_{i=1}^{p-2}\\frac{d_{i+1}}{2^i} + \\frac{1}{2^{p-1}},\n",
    "\\quad\n",
    "[y] = 1 + \\sum_{i=1}^{p-2}\\frac{d_{i+1}}{2^i} + \\frac{0}{2^{p-1}}\n",
    ".\n",
    "$$\n",
    "\n",
    "* $[x]-[y] = \\frac{1}{2^{p-1}} = [[x]-[y]]$.\n",
    "\n",
    "* The true difference $x-y$ may lie anywhere in $(0, \\frac{1}{2^{p-2}}+\\frac{1}{2^{2p}}]$.\n",
    "\n",
    "* Relative error \n",
    "$$\n",
    "\\frac{|x-y -[[x]-[y]]|}{|x-y|}\n",
    "$$\n",
    "is unbounded -- no guarantee of any significant digit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Implications for numerical computation\n",
    "    - Rule 1: add small numbers together before adding larger ones  \n",
    "    - Rule 2: add numbers of like magnitude together (paring). When all numbers are of same sign and similar magnitude, add in pairs so each stage the summands are of similar magnitude  \n",
    "    - Rule 3: avoid substraction of two numbers that are nearly equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algebraic laws\n",
    "\n",
    "Floating-point numbers may violate many algebraic laws we are familiar with, such associative and distributive laws. See the example in the Machine Epsilon section and HW1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coditioning\n",
    "\n",
    "Condiser solving a linear system $Ax=b$:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} 1.000 & 0.500 \\\\ 0.667 & 0.333 \\end{bmatrix}\n",
    "\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} 1.500 \\\\ 1.000 \\end{bmatrix}\n",
    ",\n",
    "$$\n",
    "\n",
    "whose solution is $(x_1, x_2) = (1.000, 1.000)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=[1.0 0.5; 0.667 0.333]; b = [1.5, 1.0]; A\\b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we *perturb* $b$ by 0.001, i.e., solve\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} 1.000 & 0.500 \\\\ 0.667 & 0.333 \\end{bmatrix}\n",
    "\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} 1.500 \\\\ 0.999 \\end{bmatrix}\n",
    ",\n",
    "$$\n",
    "\n",
    "then the solution changes to $(x_1, x_2) = (0.000, 3.000)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = [1.5, 0.999]; A\\b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we instead perturb $A$ by 0.001, i.e., solve\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} 1.000 & 0.500 \\\\ 0.667 & 0.334 \\end{bmatrix}\n",
    "\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} 1.500 \\\\ 1.000 \\end{bmatrix}\n",
    ",\n",
    "$$\n",
    "\n",
    "then this time the solution changes to $(x_1, x_2) = (2.000, -1.000)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1=[1.0 0.5; 0.667 0.334]; A1\\b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, an input perturbation of order $10^{-3}$ yield an output perturbation of order $10^0$. Thats 3 orders of magnutides of relative change!\n",
    "\n",
    "Floating point representation $[x]$ of a real number $x$ in a digital computer may introduce such input perturbation easily. The perturbation of output of a problem with respect to the input is called *conditioning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Abstractly, a *problem* can be viewed as function $f: X \\to Y$ where $X$ is a normed vector space of data and $Y$ is a normed vector space of solutions.\n",
    "    - The problem of solving $Ax=b$ for fixed $b$ is $f: A \\mapsto A^{-1}b$ with $X=\\{M\\in\\mathbb{R}^{n\\times n}: M \\text{ is invertible} \\}$ and $Y = \\mathbb{R}^n$.\n",
    "    - The combination of a problem $f$ with a given data $x$ is called a *problem instance*, or simply problem unless no confusion occurs.\n",
    "    \n",
    "* A *well-conditioned* problem (instance) is one such that all small perturbations of $x$ lead to only small changes in $f(x)$.\n",
    "\n",
    "* An *ill-conditioned* problem is one such that some small perturbation of $x$ leads to a large change in $f(x)$.\n",
    "\n",
    "* The (relative) *condition number* $\\kappa=\\kappa(x)$ of a problem is defined by\n",
    "$$\n",
    "    \\kappa = \\lim_{\\delta\\to 0}\\sup_{\\|\\delta x\\|\\le \\delta}\\frac{\\|\\delta f\\|/\\|f(x)\\|}{\\|\\delta x\\|/\\|x\\|}\n",
    "    = \\sup_{\\delta x} \\frac{\\|\\delta f\\|/\\|f(x)\\|}{\\|\\delta x\\|/\\|x\\|}\n",
    "$$\n",
    "\n",
    "* For the problem of solving $Ax=b$ for fixed $b$,  $f: A \\mapsto A^{-1}b$, it can be shown that the condition number of $f$ is\n",
    "$$\n",
    "    \\kappa = \\|A\\|\\|A^{-1}\\| =: \\kappa(A)\n",
    "    ,\n",
    "$$\n",
    "where $\\|A\\|$ is the matrix norm induced by vector norm $\\|\\cdot\\|$, i.e.,\n",
    "$$\n",
    "    \\|A\\| = \\sup_{x\\neq 0} \\frac{\\|Ax\\|}{\\|x\\|}.\n",
    "$$\n",
    "If 2-norm is used, then \n",
    "$$\n",
    "\\kappa(A) = \\sigma_{\\max}(A)/\\sigma_{\\min}(A),\n",
    "$$\n",
    "the ratio of the maximum and minimum singular values of $A$. To see this, let $x=A^{-1}b$. Then perturbation $A \\to A+\\delta A$ yields $x \\to x + \\delta x$. If both are small,\n",
    "$$\n",
    "(\\delta A)x + A(\\delta x) = 0\n",
    "$$\n",
    "or $\\delta x = -A^{-1}(\\delta A) x$. Then\n",
    "$$\n",
    "\\frac{\\|\\delta x\\|/\\|x\\|}{\\|\\delta A\\|/\\|A\\|} \\le \\frac{\\|A^{-1}\\|\\|\\delta A\\|\\|A\\|}{\\|\\delta A\\|}\n",
    "= \\|A^{-1}\\|\\|A\\|.\n",
    "$$\n",
    "The inequality holds with equality if $\\delta A$ satisfies \n",
    "$$\n",
    "\\|A^{-1}(\\delta A)x \\| = \\|A^{-1}\\|\\|\\delta A\\|\\|x\\|\n",
    ".\n",
    "$$\n",
    "It can be shown that for any invertible $A$ and $b$, such perturbation $\\delta A$ exists (HW2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above problem, the condition number is matrix $A$ (w.r.t. Euclidean norm) is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "LinearAlgebra.cond(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further readings\n",
    "\n",
    "* Section II.2, [Computational Statistics](https://link.springer.com/book/10.1007%2F978-0-387-98144-4) by James Gentle (2009).\n",
    "\n",
    "* Sections 1.5 and 2.2, [Applied Numerical Linear Algebra](https://doi.org/10.1137/1.9781611971446) by James W. Demmel (1997).\n",
    "\n",
    "* [What every computer scientist should know about floating-point arithmetic](https://www.itu.dk/~sestoft/bachelor/IEEE754_article.pdf) by David Goldberg (1991)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgment\n",
    "\n",
    "Many parts of this lecture note is based on [Dr. Hua Zhou](http://hua-zhou.github.io)'s 2019 Spring Statistical Computing course notes available at <http://hua-zhou.github.io/teaching/biostatm280-2019spring/index.html>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.0",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "250px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
